# -*- coding: utf-8 -*-
"""suc_leann_ollama_HF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ODMWxuHcoQJ9AXQtWhlSQucMzbFp3cSE
"""







"""https://github.com/yichuan-w/LEANN/blob/main/docs/configuration-guide.md"""

!git clone https://github.com/yichuan-w/LEANN.git leann

# Commented out IPython magic to ensure Python compatibility.
# %cd leann

!uv pip install leann

!uv pip install ollama

curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &
!ollama pull gemma3:1b

!ollama pull embeddinggemma

!ollama list

from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path("./").resolve() / "demo.leann")

from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path("./").resolve() / "demo.leann")

builder = LeannBuilder(backend_name="hnsw",
                       embedding_model="embeddinggemma:latest",
                       embedding_mode="ollama",
                       gragh_degree=32,
                       build_complexity=64
                        )
builder.add_text("Machine learning transforms industries")
builder.add_text("Neural networks process complex data")
builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
builder.build_index(INDEX_PATH)
print("Index built with embeddinggemma:latest (ollama)")
from leann.api import LeannSearcher

searcher = LeannSearcher(
    INDEX_PATH,
    embedding_model="embeddinggemma:latest",
    embedding_mode="ollama",
    search_complexity=32
    )
result = searcher.search("What is Leann?", top_k=1)

print("Search result:")
if result:
  print(result[0].text)
else:
  print("No match found.")
print("Structure of result:", result)


chat = LeannChat(
    INDEX_PATH,
    llm_config={"type": "ollama", "model": "qwen3:4b"},
    embedding_model="embeddinggemma:latest",
    embedding_mode="ollama",
    chat_complexity=32
    )
response = chat.ask("What is Leann?",top_k=1)
print("Answer:", response)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/leann

from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path("./").resolve() / "demo.leann")

builder = LeannBuilder(backend_name="hnsw",
                       embedding_model="embeddinggemma:latest",
                       embedding_mode="ollama"
                        )
builder.add_text("Machine learning transforms industries")
builder.add_text("Neural networks process complex data")
builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
builder.build_index(INDEX_PATH)
print("Index built with embeddinggemma:latest (ollama)")
from leann.api import LeannSearcher

searcher = LeannSearcher(
    INDEX_PATH,
    embedding_model="embeddinggemma:latest",
    embedding_mode="ollama"
    )
result = searcher.search("What is Leann?", top_k=1)

print("Search result:")
if result:
  print(result[0].text)
else:
  print("No match found.")
print("Structure of result:", result)


chat = LeannChat(
    INDEX_PATH,
    llm_config={"type": "ollama", "model": "qwen3:4b"},
    embedding_model="embeddinggemma:latest",
    embedding_mode="ollama"
    )
response = chat.ask("What is Leann?", top_k=1)
print("Answer:", response)

!ollama pull gemma3:1b

!ollama list

from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path("./").resolve() / "demo.leann")

builder = LeannBuilder(backend_name="hnsw",
                       embedding_model="embeddinggemma:latest",
                       embedding_mode="ollama"
                        )
builder.add_text("Machine learning transforms industries")
builder.add_text("Neural networks process complex data")
builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
builder.build_index(INDEX_PATH)
print("Index built with embeddinggemma:latest (ollama)")
from leann.api import LeannSearcher

searcher = LeannSearcher(
    INDEX_PATH,
    embedding_model="embeddinggemma:latest",
    embedding_mode="ollama"
    )
result = searcher.search("What is Leann?", top_k=1)

print("Search result:")
if result:
  print(result[0].text)
else:
  print("No match found.")
print("Structure of result:", result)


chat = LeannChat(
    INDEX_PATH,
    llm_config={"type": "ollama", "model": "gemma3:1b"},
    embedding_model="embeddinggemma:latest",
    embedding_mode="ollama"
    )
response = chat.ask("What is Leann?", top_k=1)
print("Answer:", response)

from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path("./").resolve() / "demo.leann")

builder = LeannBuilder(backend_name="hnsw",
                       embedding_model="embeddinggemma:latest",
                       embedding_mode="ollama"
                        )

builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
builder.build_index(INDEX_PATH)
print("Index built with embeddinggemma:latest (ollama)")
from leann.api import LeannSearcher

searcher = LeannSearcher(
    INDEX_PATH,
    embedding_model="embeddinggemma:latest",
    embedding_mode="ollama"
    )
result = searcher.search("What is Leann?", top_k=1)

print("Search result:")
if result:
  print(result[0].text)
else:
  print("No match found.")
print("Structure of result:", result)


chat = LeannChat(
    INDEX_PATH,
    llm_config={"type": "ollama", "model": "gemma3:1b"},
    embedding_model="embeddinggemma:latest",
    embedding_mode="ollama"
    )
response = chat.ask("What is Leann?", top_k=1)
print("Answer:", response)

!leann build mydemo --docs ./data --force

!leann search mydemo "What is python?"

!leann -h

!leann ask mydemo What is python?

# Commented out IPython magic to ensure Python compatibility.
# Docstring:
# %%python script magic
# 
# Run cells with python in a subprocess.
# 
# This is a shortcut for `%%script python`
# File:      /usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py

# Commented out IPython magic to ensure Python compatibility.
# %%python script magic
# !leann ask mydemo "What is python?"

print("Hello from a Python subprocess!")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

# Commented out IPython magic to ensure Python compatibility.
# %%writefile run_leann.py
# 
# # كل الكود الذي في هذه الخلية سيتم حفظه في ملف run_leann.py
# from leann import LeannBuilder, LeannSearcher, LeannChat
# from pathlib import Path
# import os
# 
# # تأكد من تثبيت المكتبات الضرورية أولاً
# # !pip install leann-python ollama
# 
# # إنشاء مسار للفهرس
# INDEX_PATH = str(Path("./").resolve() / "demo.leann")
# 
# # حذف الفهرس القديم إذا كان موجودًا لضمان البدء من جديد
# if os.path.exists(INDEX_PATH):
#     import shutil
#     shutil.rmtree(INDEX_PATH)
#     print(f"Removed old index at: {INDEX_PATH}")
# 
# 
# print("Building index...")
# builder = LeannBuilder(backend_name="hnsw",
#                        embedding_model="embeddinggemma:latest",
#                        embedding_mode="ollama"
#                         )
# builder.add_text("Machine learning transforms industries")
# builder.add_text("Neural networks process complex data")
# builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
# builder.build_index(INDEX_PATH)
# print("Index built with embeddinggemma:latest (ollama)")
# 
# 
# print("\nSearching...")
# searcher = LeannSearcher(
#     INDEX_PATH,
#     embedding_model="embeddinggemma:latest",
#     embedding_mode="ollama"
#     )
# result = searcher.search("What is Leann?", top_k=1)
# 
# print("Search result:")
# if result:
#   print(result[0].text)
# else:
#   print("No match found.")
# print("Structure of result:", result)
# 
# 
# print("\nChatting...")
# chat = LeannChat(
#     INDEX_PATH,
#     llm_config={"type": "ollama", "model": "qwen3:4b"},
#     embedding_model="embeddinggemma:latest",
#     embedding_mode="ollama"
#     )
# response = chat.ask("What is Leann?", top_k=1)
# print("Answer:", response)
# 
# print("\nScript finished.")

!python /content/run_leann.py

!leann ask mydemo \
  --llm ollama \
  --llm-model gemma3:1b \
  --host http://127.0.0.1:11434

!leann ask mydemo --llm-model ollama/gemma3:1b

!leann ask -h

usage: leann ask [-h] [--llm {simulated,ollama,hf,openai}] [--model MODEL]
                 [--host HOST] [--interactive] [--top-k TOP_K]
                 [--complexity COMPLEXITY] [--beam-width BEAM_WIDTH]
                 [--prune-ratio PRUNE_RATIO] [--recompute | --no-recompute]
                 [--pruning-strategy {global,local,proportional}]
                 [--thinking-budget {low,medium,high}]
                 index_name

positional arguments:
  index_name            Index name

options:
  -h, --help            show this help message and exit
  --llm {simulated,ollama,hf,openai}
                        LLM provider (default: ollama)
  --model MODEL         Model name (default: qwen3:8b)
  --host HOST
  --interactive, -i     Interactive chat mode
  --top-k TOP_K         Retrieval count (default: 20)
  --complexity COMPLEXITY
  --beam-width BEAM_WIDTH
  --prune-ratio PRUNE_RATIO
  --recompute, --no-recompute
                        Enable/disable embedding recomputation during ask
                        (default: enabled)
  --pruning-strategy {global,local,proportional}
  --thinking-budget {low,medium,high}
                        Thinking budget for reasoning models
                        (low/medium/high). Supported by GPT-Oss:20b and other
                        reasoning models.

!leann ask mydemo --llm ollama --model gemma3:1b --top-k 1  -i

!leann ask mydemo --llm ollama --model gemma3:1b  -i

!leann ask mydemo --llm hf --model Qwen/Qwen3-0.6B --top-k 1  -i