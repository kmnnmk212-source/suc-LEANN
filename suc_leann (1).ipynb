{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "571e11c80c9e4b558b20699b9491278b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0d0b579e150473aab1f6d3a2a962d9a",
              "IPY_MODEL_e5d94c764bd644ee931e1f527079ab0b",
              "IPY_MODEL_f8ba5a4d348e4b78ad24575ebeecc96e"
            ],
            "layout": "IPY_MODEL_584142b9218e40098a57444df4bbaf72"
          }
        },
        "d0d0b579e150473aab1f6d3a2a962d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5d38c1b88294888a28c6ba09cd87f82",
            "placeholder": "​",
            "style": "IPY_MODEL_f85cb16dd87e48788d93c620bd33fbde",
            "value": "Batches: 100%"
          }
        },
        "e5d94c764bd644ee931e1f527079ab0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88ce85296b774bd1aa7552df901c9b8f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a64ee4cdb6a438981b1528ca3d4bce3",
            "value": 1
          }
        },
        "f8ba5a4d348e4b78ad24575ebeecc96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aba10ceb1ba246e09bce496f31939052",
            "placeholder": "​",
            "style": "IPY_MODEL_f10095ca0f3c4f2b946c514693d42374",
            "value": " 1/1 [00:14&lt;00:00, 14.00s/it]"
          }
        },
        "584142b9218e40098a57444df4bbaf72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5d38c1b88294888a28c6ba09cd87f82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f85cb16dd87e48788d93c620bd33fbde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88ce85296b774bd1aa7552df901c9b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a64ee4cdb6a438981b1528ca3d4bce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aba10ceb1ba246e09bce496f31939052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f10095ca0f3c4f2b946c514693d42374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3dcb69343a3e4f2580c644e0605c93f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f4c6ef1c8bd43c090d53abde8ccf012",
              "IPY_MODEL_e671e6f5bd184ea98638d481066e5644",
              "IPY_MODEL_e573b2286ea2419e8696f3d7e7c38521"
            ],
            "layout": "IPY_MODEL_d402a69bc58c45f9bc0a7bc06f8dd689"
          }
        },
        "5f4c6ef1c8bd43c090d53abde8ccf012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b7e1658076e469db2d3774d05ed3217",
            "placeholder": "​",
            "style": "IPY_MODEL_ff4d4253d2944914bce7b0d0b8a40d31",
            "value": "Batches: 100%"
          }
        },
        "e671e6f5bd184ea98638d481066e5644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b4a73dde6124308b5f6961a8bd5746d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26a22c829fb7473695926925466eaeaf",
            "value": 1
          }
        },
        "e573b2286ea2419e8696f3d7e7c38521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fdb0f0594bd44e484195f19bc14adc0",
            "placeholder": "​",
            "style": "IPY_MODEL_f30e5899459841c883a6ef378416cf01",
            "value": " 1/1 [00:14&lt;00:00, 14.71s/it]"
          }
        },
        "d402a69bc58c45f9bc0a7bc06f8dd689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b7e1658076e469db2d3774d05ed3217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff4d4253d2944914bce7b0d0b8a40d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b4a73dde6124308b5f6961a8bd5746d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26a22c829fb7473695926925466eaeaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5fdb0f0594bd44e484195f19bc14adc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f30e5899459841c883a6ef378416cf01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f68b9577a6b24da8a0a1f74bd9243a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd3a65f91ffa446a98b3a54f596d73a9",
              "IPY_MODEL_50e5bf1eb4424c65b74fe4a296510988",
              "IPY_MODEL_57183327e06e4118908a29c6ff12220a"
            ],
            "layout": "IPY_MODEL_eaa9281f8b0d4a0aa4b9b24269bc2db4"
          }
        },
        "cd3a65f91ffa446a98b3a54f596d73a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eba1d81c5ec4343b452555e45e92cc5",
            "placeholder": "​",
            "style": "IPY_MODEL_fd0a5e3d32ab44e880f5350c8634ece4",
            "value": "Batches: 100%"
          }
        },
        "50e5bf1eb4424c65b74fe4a296510988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_babd63f76633418eb445d6c130870ede",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ba6f13b721c486f9cc18a2ed3143b02",
            "value": 3
          }
        },
        "57183327e06e4118908a29c6ff12220a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e0584951ff49d6a97cb8aec1ef51a6",
            "placeholder": "​",
            "style": "IPY_MODEL_7fd909ab7893403cabb5ee84cf0b1032",
            "value": " 3/3 [07:02&lt;00:00, 131.38s/it]"
          }
        },
        "eaa9281f8b0d4a0aa4b9b24269bc2db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eba1d81c5ec4343b452555e45e92cc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd0a5e3d32ab44e880f5350c8634ece4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "babd63f76633418eb445d6c130870ede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ba6f13b721c486f9cc18a2ed3143b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39e0584951ff49d6a97cb8aec1ef51a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fd909ab7893403cabb5ee84cf0b1032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d7a4ca4ddd64bc0a5b162f21cb9c7f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f709cedaf4940a68dbe3b9416a9d55d",
              "IPY_MODEL_958fbb8b590d49898eb8540028cc8e39",
              "IPY_MODEL_5ba7c2c5ccef4147b95aa493f40e1147"
            ],
            "layout": "IPY_MODEL_ae1b6cc82bd84d2ebda5c6ad687223d3"
          }
        },
        "6f709cedaf4940a68dbe3b9416a9d55d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b14c97988a64200821decdbcaf5ae2f",
            "placeholder": "​",
            "style": "IPY_MODEL_ad1c6bb1f7f54181b979c4cd1166431a",
            "value": "Batches: 100%"
          }
        },
        "958fbb8b590d49898eb8540028cc8e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75a090289ace461b9ebc95ddbbf8a6b5",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2565dca9f04540c78d847e0e19af26bc",
            "value": 3
          }
        },
        "5ba7c2c5ccef4147b95aa493f40e1147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd900446bcd8435a8803f7cddcac8161",
            "placeholder": "​",
            "style": "IPY_MODEL_933774f1714f4ed6b0daafc337182275",
            "value": " 3/3 [06:51&lt;00:00, 127.83s/it]"
          }
        },
        "ae1b6cc82bd84d2ebda5c6ad687223d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b14c97988a64200821decdbcaf5ae2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad1c6bb1f7f54181b979c4cd1166431a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75a090289ace461b9ebc95ddbbf8a6b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2565dca9f04540c78d847e0e19af26bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd900446bcd8435a8803f7cddcac8161": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "933774f1714f4ed6b0daafc337182275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eKBVIU6WkYW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aWqzlDy0WkzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lU6iDjV89kyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G5FhjXXi9k1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PnJN61So9k4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/leann/data/PrideandPrejudice.txt\n",
        "النص\n",
        "\n",
        "Title: Pride and Prejudice\n",
        "\n",
        "Author: Jane Austen\n",
        "\n",
        "Release date: June 1, 1998 [eBook #1342]\n",
        "                Most recently updated: October 29, 2024\n",
        "\n",
        "Language: English"
      ],
      "metadata": {
        "id": "80dWRfdk9k7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغاله"
      ],
      "metadata": {
        "id": "I1MkB_0p9x29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!leann build my-docs --docs ./data --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022df64f-85c0-4fd1-aaa0-7bcb61ee790d",
        "id": "P_IikWZE9x2_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Indexing 1 path:\n",
            "  📁 Directories (1):\n",
            "    1. /content/leann/data\n",
            "Loading documents from 1 directory (1 total):\n",
            "  📁 Directories: data\n",
            "\n",
            "🔄 Processing 1 directory...\n",
            "Processing directory: data\n",
            "📋 No .gitignore found\n",
            "Loading files: 100% 1/1 [00:00<00:00,  2.74it/s]\n",
            "Loaded 1 documents from data\n",
            "start chunking documents\n",
            "Chunking documents: 100% 1/1 [00:00<00:00, 852.33doc/s]\n",
            "Loaded 1 documents, 1 chunks\n",
            "Building index 'my-docs' with hnsw backend...\n",
            "2025-09-25 02:53:53.258429: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758768833.283103   31229 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758768833.290696   31229 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758768833.310059   31229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758768833.310101   31229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758768833.310106   31229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758768833.310112   31229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-25 02:53:53.315576: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Writing passages: 100% 1/1 [00:00<00:00, 11586.48chunk/s]\n",
            "Batches: 100% 1/1 [00:00<00:00,  1.85it/s]\n",
            "WARNING:leann_backend_hnsw.hnsw_backend:Converting data to float32, shape: (1, 768)\n",
            "M: 64 for level: 0\n",
            "Starting conversion: /content/leann/.leann/indexes/my-docs/documents.index -> /content/leann/.leann/indexes/my-docs/documents.csr.tmp\n",
            "[0.00s] Reading Index HNSW header...\n",
            "[0.00s]   Header read: d=768, ntotal=1\n",
            "[0.00s] Reading HNSW struct vectors...\n",
            "  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48\n",
            "[0.00s]   Read assign_probas (6)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28\n",
            "[0.48s]   Read cum_nneighbor_per_level (7)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=1, Bytes=4\n",
            "[0.95s]   Read levels (1)\n",
            "[1.55s]   Probing for compact storage flag...\n",
            "[1.55s]   Found compact flag: False\n",
            "[1.55s]   Compact flag is False, reading original format...\n",
            "[1.55s]   Probing for potential extra byte before non-compact offsets...\n",
            "[1.55s]   Found and consumed an unexpected 0x00 byte.\n",
            "  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=2, Bytes=16\n",
            "[1.55s]   Read offsets (2)\n",
            "[2.14s]   Attempting to read neighbors vector...\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=64, Bytes=256\n",
            "[2.14s]   Read neighbors (64)\n",
            "[2.74s]   Read scalar params (ep=0, max_lvl=0)\n",
            "[2.74s] Checking for storage data...\n",
            "[2.74s]   Found storage fourcc: 49467849.\n",
            "[2.74s] Converting to CSR format...\n",
            "[2.74s]   Conversion loop finished.                        \n",
            "[2.74s] Running validation checks...\n",
            "    Checking total valid neighbor count...\n",
            "    OK: Total valid neighbors = 0\n",
            "    Checking final pointer indices...\n",
            "    OK: Final pointers match data size.\n",
            "[2.74s] Deleting original neighbors and offsets arrays...\n",
            "    CSR Stats: |data|=0, |level_ptr|=2\n",
            "[3.21s] Writing CSR HNSW graph data in FAISS-compatible order...\n",
            "   Pruning embeddings: Writing NULL storage marker.\n",
            "[3.63s] Conversion complete.\n",
            "Index built at /content/leann/.leann/indexes/my-docs/documents.leann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغالة"
      ],
      "metadata": {
        "id": "_zonKgKf9lwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!leann search my-docs \"author\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "043a9bf4-a081-418b-843c-9f5d5fb3946f",
        "id": "UlbucNJ39lwH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
            "[read_HNSW NL v4] Read levels vector, size: 1\n",
            "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
            "[read_HNSW NL v4] Read compact_level_ptr, size: 2\n",
            "[read_HNSW NL v4] Read compact_node_offsets, size: 2\n",
            "[read_HNSW NL v4] Read entry_point: 0, max_level: 0\n",
            "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
            "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 214\n",
            "[read_HNSW NL v4] Reading neighbors data into memory.\n",
            "[read_HNSW NL v4] Read neighbors data, size: 0\n",
            "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
            "INFO: Skipping external storage loading, since is_recompute is true.\n",
            "WARNING:leann.api:  ⚠️  Requested top_k (5) exceeds total documents (1)\n",
            "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
            "ZmqDistanceComputer initialized: d=768, metric=0\n",
            "Search results for 'author' (top 1):\n",
            "1. Score: 0.541\n",
            "   Title: Pride and Prejudice\n",
            "\n",
            "Author: Jane Austen\n",
            "\n",
            "Release date: June 1, 1998 [eBook #1342]\n",
            "                Most recently updated: October 29, 2024\n",
            "\n",
            "Language: English...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# الصيغة تللك تعمل **جيدا**"
      ],
      "metadata": {
        "id": "o8RkrjSY9lwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs --model llama3.2:1b --interactive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11436f9-ba16-4ca5-df3b-aba41612c470",
        "id": "qapCj5nt9lwJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chat with index '/content/leann/.leann/indexes/my-docs'...\n",
            "Using llama3.2:1b (ollama)\n",
            "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
            "[read_HNSW NL v4] Read levels vector, size: 1\n",
            "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
            "[read_HNSW NL v4] Read compact_level_ptr, size: 2\n",
            "[read_HNSW NL v4] Read compact_node_offsets, size: 2\n",
            "[read_HNSW NL v4] Read entry_point: 0, max_level: 0\n",
            "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
            "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 214\n",
            "[read_HNSW NL v4] Reading neighbors data into memory.\n",
            "[read_HNSW NL v4] Read neighbors data, size: 0\n",
            "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
            "INFO: Skipping external storage loading, since is_recompute is true.\n",
            "LEANN Assistant ready! Type 'quit' to exit\n",
            "========================================\n",
            "\n",
            "You: What is the author’s name\n",
            "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
            "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
            "ZmqDistanceComputer initialized: d=768, metric=0\n",
            "LEANN: Based on the provided context, I can confidently answer that the author's name is Jane Austen. The fact that it's an eBook with a release date (1998) and updates (2024), along with being part of a larger literary work (Pride and Prejudice), strongly suggests that it is indeed written by Jane Austen.\n",
            "\n",
            "You: Show me the existing text.\n",
            "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
            "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
            "ZmqDistanceComputer initialized: d=768, metric=0\n",
            "LEANN: Based on the provided context, I will attempt to display the existing text of \"Pride and Prejudice\" by Jane Austen:\n",
            "\n",
            "\"Not for the last time, Miss Bingley has been the object of my jealousy; but at present I am not so much affected by her beauty as by her good sense. Her father is a man of some property, but he is not in every respect advantageous to her family; though his abilities are better than his heart, and many people consider him great. His sister Jane is very pretty, and an excellent musician; she has a tolerable voice, which can scarce be expected to captivate the heart, but it may influence it. Her character is of a middle sort; not handsome, but not ugly; serious, reserved, and not very attractive; though her personal beauty, as I said before, is quite lost on me.\"\n",
            "\n",
            "Please note that this is an excerpt from the 1998 eBook edition of \"Pride and Prejudice\" with the most recent update being October 29, 2024.\n",
            "\n",
            "You: what is Release date?\n",
            "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
            "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
            "ZmqDistanceComputer initialized: d=768, metric=0\n",
            "LEANN: Based on the provided context, I'm going to take a close look at the information.\n",
            "\n",
            "The release date mentioned in the question is June 1, 1998. \n",
            "\n",
            "However, according to the text, the book was \"most recently updated\" on October 29, 2024. This implies that there have been changes or updates made to the text since the original publication of the book, specifically after January 1, 2025 (not 2024).\n",
            "\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lObGBVxYC7lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dj_0boVzC7hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yDri9YhLC7Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "cECFeVPuC8Tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!leann build my-notes --docs ./data --embedding-mode ollama --embedding-model nomic-embed-text:latest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd333a4-734b-4ec7-94fe-422fc39155d6",
        "id": "WvG_8OJPC8Tn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Indexing 1 path:\n",
            "  📁 Directories (1):\n",
            "    1. /content/leann/data\n",
            "Loading documents from 1 directory (1 total):\n",
            "  📁 Directories: data\n",
            "\n",
            "🔄 Processing 1 directory...\n",
            "Processing directory: data\n",
            "📋 No .gitignore found\n",
            "Loading files: 100% 1/1 [00:00<00:00,  1.30it/s]\n",
            "Loaded 1 documents from data\n",
            "start chunking documents\n",
            "Chunking documents: 100% 1/1 [00:00<00:00, 459.30doc/s]\n",
            "Loaded 1 documents, 1 chunks\n",
            "Building index 'my-notes' with hnsw backend...\n",
            "Writing passages: 100% 1/1 [00:00<00:00, 8738.13chunk/s]\n",
            "Computing Ollama embeddings: 100% 1/1 [00:00<00:00,  3.20it/s]\n",
            "M: 64 for level: 0\n",
            "Starting conversion: /content/leann/.leann/indexes/my-notes/documents.index -> /content/leann/.leann/indexes/my-notes/documents.csr.tmp\n",
            "[0.00s] Reading Index HNSW header...\n",
            "[0.00s]   Header read: d=768, ntotal=1\n",
            "[0.00s] Reading HNSW struct vectors...\n",
            "  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48\n",
            "[0.00s]   Read assign_probas (6)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28\n",
            "[0.23s]   Read cum_nneighbor_per_level (7)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=1, Bytes=4\n",
            "[0.44s]   Read levels (1)\n",
            "[0.66s]   Probing for compact storage flag...\n",
            "[0.66s]   Found compact flag: False\n",
            "[0.66s]   Compact flag is False, reading original format...\n",
            "[0.66s]   Probing for potential extra byte before non-compact offsets...\n",
            "[0.66s]   Found and consumed an unexpected 0x00 byte.\n",
            "  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=2, Bytes=16\n",
            "[0.66s]   Read offsets (2)\n",
            "[0.87s]   Attempting to read neighbors vector...\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=64, Bytes=256\n",
            "[0.87s]   Read neighbors (64)\n",
            "[1.08s]   Read scalar params (ep=0, max_lvl=0)\n",
            "[1.08s] Checking for storage data...\n",
            "[1.08s]   Found storage fourcc: 49467849.\n",
            "[1.08s] Converting to CSR format...\n",
            "[1.08s]   Conversion loop finished.                        \n",
            "[1.08s] Running validation checks...\n",
            "    Checking total valid neighbor count...\n",
            "    OK: Total valid neighbors = 0\n",
            "    Checking final pointer indices...\n",
            "    OK: Final pointers match data size.\n",
            "[1.08s] Deleting original neighbors and offsets arrays...\n",
            "    CSR Stats: |data|=0, |level_ptr|=2\n",
            "[1.30s] Writing CSR HNSW graph data in FAISS-compatible order...\n",
            "   Pruning embeddings: Writing NULL storage marker.\n",
            "[1.53s] Conversion complete.\n",
            "Index built at /content/leann/.leann/indexes/my-notes/documents.leann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغاله"
      ],
      "metadata": {
        "id": "RiXTj5_xC83z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask my-notes --llm ollama  --model llama3.2:1b -i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a1bfd5-6ea5-4fb6-c133-9cc7b9be3601",
        "id": "xtxz2q9mC8To"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chat with index 'my-notes'...\n",
            "Using llama3.2:1b (ollama)\n",
            "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
            "[read_HNSW NL v4] Read levels vector, size: 1\n",
            "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
            "[read_HNSW NL v4] Read compact_level_ptr, size: 2\n",
            "[read_HNSW NL v4] Read compact_node_offsets, size: 2\n",
            "[read_HNSW NL v4] Read entry_point: 0, max_level: 0\n",
            "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
            "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 214\n",
            "[read_HNSW NL v4] Reading neighbors data into memory.\n",
            "[read_HNSW NL v4] Read neighbors data, size: 0\n",
            "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
            "INFO: Skipping external storage loading, since is_recompute is true.\n",
            "LEANN Assistant ready! Type 'quit' to exit\n",
            "========================================\n",
            "\n",
            "You: What is the author’s name\n",
            "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
            "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
            "ZmqDistanceComputer initialized: d=768, metric=0\n",
            "LEANN: Based on the provided context, I can confidently state that the author's name is Jane Austen.\n",
            "\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "reBfNxedEeTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "3hJL6DcwEgC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!leann build my-index --force --no-recompute --no-compact\n",
        "\n",
        "\n",
        "from leann import LeannSearcher\n",
        "\n",
        "searcher = LeannSearcher(\"/content/leann/.leann/indexes\")\n",
        "results = searcher.search(\"What is the author’s name\", top_k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0bb5c7-59f3-4d64-a5a9-470aeb41da0f",
        "id": "gqBYp4agEfFN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Indexing 1 path:\n",
            "  📁 Directories (1):\n",
            "    1. /content/leann\n",
            "Loading documents from 1 directory (1 total):\n",
            "  📁 Directories: .\n",
            "\n",
            "🔄 Processing 1 directory...\n",
            "Processing directory: .\n",
            "📋 Loaded .gitignore from . (includes all subdirectories)\n",
            "Loading files: 100% 116/116 [00:03<00:00, 30.27it/s]\n",
            "Loaded 109 documents from .\n",
            "start chunking documents\n",
            "Chunking documents: 100% 109/109 [00:02<00:00, 39.37doc/s]\n",
            "Loaded 109 documents, 1787 chunks\n",
            "Building index 'my-index' with hnsw backend...\n",
            "Warning: Skipping 4 empty/invalid text chunk(s). Processing 1783 valid chunks\n",
            "2025-09-25 04:08:01.889121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758773281.914566   49202 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758773281.922073   49202 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758773281.943792   49202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758773281.943839   49202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758773281.943845   49202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758773281.943850   49202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-25 04:08:01.950196: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Writing passages: 100% 1783/1783 [00:00<00:00, 35370.78chunk/s]\n",
            "Batches:   0% 0/56 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs --model llama3.2:1b --interactive\n",
        "\n",
        "\n",
        "Starting chat with index '/content/leann/.leann/indexes/my-docs'...\n",
        "Using llama3.2:1b (ollama)\n",
        "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
        "[read_HNSW NL v4] Read levels vector, size: 1\n",
        "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
        "[read_HNSW NL v4] Read compact_level_ptr, size: 2\n",
        "[read_HNSW NL v4] Read compact_node_offsets, size: 2\n",
        "[read_HNSW NL v4] Read entry_point: 0, max_level: 0\n",
        "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
        "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 214\n",
        "[read_HNSW NL v4] Reading neighbors data into memory.\n",
        "[read_HNSW NL v4] Read neighbors data, size: 0\n",
        "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
        "INFO: Skipping external storage loading, since is_recompute is true.\n",
        "LEANN Assistant ready! Type 'quit' to exit\n",
        "========================================\n",
        "\n",
        "You: What is the author’s name\n",
        "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
        "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
        "ZmqDistanceComputer initialized: d=768, metric=0\n",
        "LEANN: Based on the provided context, I can confidently answer that the author's name is Jane Austen. The fact that it's an eBook with a release date (1998) and updates (2024), along with being part of a larger literary work (Pride and Prejudice), strongly suggests that it is indeed written by Jane Austen.\n",
        "\n",
        "You: Show me the existing text.\n",
        "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
        "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
        "ZmqDistanceComputer initialized: d=768, metric=0\n",
        "LEANN: Based on the provided context, I will attempt to display the existing text of \"Pride and Prejudice\" by Jane Austen:\n",
        "\n",
        "\"Not for the last time, Miss Bingley has been the object of my jealousy; but at present I am not so much affected by her beauty as by her good sense. Her father is a man of some property, but he is not in every respect advantageous to her family; though his abilities are better than his heart, and many people consider him great. His sister Jane is very pretty, and an excellent musician; she has a tolerable voice, which can scarce be expected to captivate the heart, but it may influence it. Her character is of a middle sort; not handsome, but not ugly; serious, reserved, and not very attractive; though her personal beauty, as I said before, is quite lost on me.\"\n",
        "\n",
        "Please note that this is an excerpt from the 1998 eBook edition of \"Pride and Prejudice\" with the most recent update being October 29, 2024.\n",
        "\n",
        "You: what is Release date?\n",
        "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
        "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
        "ZmqDistanceComputer initialized: d=768, metric=0\n",
        "LEANN: Based on the provided context, I'm going to take a close look at the information.\n",
        "\n",
        "The release date mentioned in the question is June 1, 1998.\n",
        "\n",
        "However, according to the text, the book was \"most recently updated\" on October 29, 2024. This implies that there have been changes or updates made to the text since the original publication of the book, specifically after January 1, 2025 (not 2024).\n",
        "\n",
        "You: exit\n",
        "Goodbye!\n",
        "\n",
        "النص\n",
        "Title: Pride and Prejudice\n",
        "\n",
        "Author: Jane Austen\n",
        "\n",
        "Release date: June 1, 1998 [eBook #1342]\n",
        "                Most recently updated: October 29, 2024\n",
        "\n",
        "Language: English\n",
        "\n",
        "\n",
        "الصيغة دى تعمل باقى الصيغ لاتعمل"
      ],
      "metadata": {
        "id": "JoLd8MdW9k-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCBBChS_93M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5mm09AE93QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWXrK4Zf93T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJxxY4AU93Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U6hX7S2L93Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2CLAFnf793ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QBhMlvbB9lAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sCjUJXLq9lDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uw-SLu_B9lIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/yichuan-w/LEANN/tree/main"
      ],
      "metadata": {
        "id": "GoCZxUt-Z-vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت المكتبات الأساسية لـ LEANN\n",
        "!uv pip install leann-core leann-backend-hnsw --no-deps\n",
        "!uv pip install leann --no-deps\n",
        "\n",
        "# تثبيت مكتبة pypdf لقراءة ملفات PDF\n",
        "!pip install pypdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoFdgQiVWk1q",
        "outputId": "7e9f5888-be1b-458b-d290-41e2d5c48be9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 65ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 2.45s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 163ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mleann-backend-hnsw\u001b[0m\u001b[2m==0.3.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mleann-core\u001b[0m\u001b[2m==0.3.4\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 27ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 23ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mleann\u001b[0m\u001b[2m==0.3.4\u001b[0m\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.1.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading pypdf-6.1.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-6.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "from pypdf import PdfReader\n",
        "import logging\n",
        "\n",
        "# إعدادات لتسجيل معلومات أكثر تفصيلاً\n",
        "os.environ[\"LEANN_LOG_LEVEL\"] = \"INFO\"\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# تحديد مسار لحفظ الفهرس\n",
        "INDEX_DIR = Path(\"./\").resolve()\n",
        "INDEX_PATH = str(INDEX_DIR / \"demo.leann\")\n",
        "\n",
        "print(\"تم تجهيز الإعدادات بنجاح.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyKYzDWrWo3i",
        "outputId": "cdb2bce5-6d5f-4ff0-c50d-dbcce3ca3452"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تم تجهيز الإعدادات بنجاح.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"الرجاء اختيار ملف PDF لرفعه...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# التأكد من أنه تم رفع ملف واحد فقط واستخلاص اسمه\n",
        "if len(uploaded) == 0:\n",
        "    print(\"لم يتم رفع أي ملف.\")\n",
        "elif len(uploaded) > 1:\n",
        "    print(\"الرجاء رفع ملف واحد فقط.\")\n",
        "else:\n",
        "    pdf_name = next(iter(uploaded))\n",
        "    print(f\"\\nتم رفع الملف '{pdf_name}' بنجاح.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "S0XIT3N9Wq7T",
        "outputId": "4924b161-65d2-4053-c9ca-6e4f124a8d12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "الرجاء اختيار ملف PDF لرفعه...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1462c227-8b03-4585-82ad-13e4d6ab1a64\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1462c227-8b03-4585-82ad-13e4d6ab1a64\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Understanding_Climate_Change.pdf to Understanding_Climate_Change (1).pdf\n",
            "\n",
            "تم رفع الملف 'Understanding_Climate_Change (1).pdf' بنجاح.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'pdf_name' in locals():\n",
        "    print(f\"جاري استخلاص النص من '{pdf_name}'...\")\n",
        "\n",
        "    # قائمة لتخزين أجزاء النص\n",
        "    text_chunks = []\n",
        "\n",
        "    try:\n",
        "        reader = PdfReader(pdf_name)\n",
        "        full_text = \"\"\n",
        "        for page in reader.pages:\n",
        "            full_text += page.extract_text() + \"\\n\"\n",
        "\n",
        "        # تقسيم النص الكامل إلى أجزاء بناءً على الفقرات (سطرين فارغين)\n",
        "        text_chunks = [chunk.strip() for chunk in full_text.split('\\n\\n') if chunk.strip()]\n",
        "\n",
        "        print(f\"تم تقسيم النص إلى {len(text_chunks)} جزء/فقرة.\")\n",
        "    except Exception as e:\n",
        "        print(f\"حدث خطأ أثناء قراءة الملف: {e}\")\n",
        "else:\n",
        "    print(\"الرجاء تشغيل خلية رفع الملف أولاً.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwICTzwNWtcS",
        "outputId": "39dfccef-e306-45f9-d321-c8e57c6d4fe2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "جاري استخلاص النص من 'Understanding_Climate_Change (1).pdf'...\n",
            "تم تقسيم النص إلى 1 جزء/فقرة.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from leann.api import LeannBuilder\n",
        "\n",
        "if text_chunks:\n",
        "    print(\"جاري بناء الفهرس من محتوى الـ PDF...\")\n",
        "    builder = LeannBuilder(backend_name=\"hnsw\")\n",
        "\n",
        "    # إضافة كل جزء نصي إلى الباني\n",
        "    for chunk in text_chunks:\n",
        "        builder.add_text(chunk)\n",
        "\n",
        "    # بناء وحفظ الفهرس في المسار المحدد\n",
        "    builder.build_index(INDEX_PATH)\n",
        "    print(f\"تم بناء الفهرس بنجاح وتم حفظه في: {INDEX_PATH}\")\n",
        "else:\n",
        "    print(\"لا يوجد نص لبناء الفهرس. الرجاء التأكد من أن ملف الـ PDF يحتوي على نص وأن الخلية السابقة عملت بنجاح.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900,
          "referenced_widgets": [
            "571e11c80c9e4b558b20699b9491278b",
            "d0d0b579e150473aab1f6d3a2a962d9a",
            "e5d94c764bd644ee931e1f527079ab0b",
            "f8ba5a4d348e4b78ad24575ebeecc96e",
            "584142b9218e40098a57444df4bbaf72",
            "b5d38c1b88294888a28c6ba09cd87f82",
            "f85cb16dd87e48788d93c620bd33fbde",
            "88ce85296b774bd1aa7552df901c9b8f",
            "7a64ee4cdb6a438981b1528ca3d4bce3",
            "aba10ceb1ba246e09bce496f31939052",
            "f10095ca0f3c4f2b946c514693d42374"
          ]
        },
        "id": "kjhUY_QsW6Aa",
        "outputId": "7ccf06c5-12d7-47ed-b36c-e23eab4766b7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'\n",
            "INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever\n",
            "INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 32, manual_tokenize=False)\n",
            "INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768\n",
            "INFO:leann.embedding_compute:Time taken: 0.07604575157165527 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "جاري بناء الفهرس من محتوى الـ PDF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Writing passages: 100%|██████████| 1/1 [00:00<00:00, 502.01chunk/s]\n",
            "INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'\n",
            "INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever\n",
            "INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 32, manual_tokenize=False)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "571e11c80c9e4b558b20699b9491278b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768\n",
            "INFO:leann.embedding_compute:Time taken: 14.011502504348755 seconds\n",
            "WARNING:leann_backend_hnsw.hnsw_backend:Converting data to float32, shape: (1, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting conversion: /content/demo.index -> /content/demo.csr.tmp\n",
            "[0.00s] Reading Index HNSW header...\n",
            "[0.00s]   Header read: d=768, ntotal=1\n",
            "[0.00s] Reading HNSW struct vectors...\n",
            "  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48\n",
            "[0.00s]   Read assign_probas (6)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28\n",
            "[0.66s]   Read cum_nneighbor_per_level (7)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=1, Bytes=4\n",
            "[1.31s]   Read levels (1)\n",
            "[1.95s]   Probing for compact storage flag...\n",
            "[1.95s]   Found compact flag: False\n",
            "[1.95s]   Compact flag is False, reading original format...\n",
            "[1.95s]   Probing for potential extra byte before non-compact offsets...\n",
            "[1.95s]   Found and consumed an unexpected 0x00 byte.\n",
            "  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=2, Bytes=16\n",
            "[1.95s]   Read offsets (2)\n",
            "[2.61s]   Attempting to read neighbors vector...\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=64, Bytes=256\n",
            "[2.61s]   Read neighbors (64)\n",
            "[3.25s]   Read scalar params (ep=0, max_lvl=0)\n",
            "[3.25s] Checking for storage data...\n",
            "[3.25s]   Found storage fourcc: 49467849.\n",
            "[3.25s] Converting to CSR format...\n",
            "[3.25s]   Conversion loop finished.                        \n",
            "[3.25s] Running validation checks...\n",
            "    Checking total valid neighbor count...\n",
            "    OK: Total valid neighbors = 0\n",
            "    Checking final pointer indices...\n",
            "    OK: Final pointers match data size.\n",
            "[3.25s] Deleting original neighbors and offsets arrays...\n",
            "    CSR Stats: |data|=0, |level_ptr|=2\n",
            "[3.88s] Writing CSR HNSW graph data in FAISS-compatible order...\n",
            "   Pruning embeddings: Writing NULL storage marker.\n",
            "[4.53s] Conversion complete.\n",
            "تم بناء الفهرس بنجاح وتم حفظه في: /content/demo.leann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from leann.api import LeannSearcher\n",
        "\n",
        "if os.path.exists(INDEX_PATH):\n",
        "    # قم بتغيير النص أدناه للبحث عن أي شيء في المستند الخاص بك\n",
        "    search_query = \"Climate Change\"\n",
        "\n",
        "    print(f\"جاري البحث عن: '{search_query}'...\")\n",
        "    searcher = LeannSearcher(INDEX_PATH)\n",
        "    results = searcher.search(search_query, top_k=2)\n",
        "\n",
        "    # عرض النتائج\n",
        "    for result in results:\n",
        "        print(f\"Score: {result.score:.4f}\")\n",
        "        print(f\"Text: {result.text}\\n\")\n",
        "else:\n",
        "    print(\"ملف الفهرس غير موجود. الرجاء تشغيل خلية بناء الفهرس أولاً.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZrTXpnsXcCe",
        "outputId": "a9a8aae0-a2e2-4d63-a7f8-c155d8bf3aa5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ملف الفهرس غير موجود. الرجاء تشغيل خلية بناء الفهرس أولاً.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Understanding Climate Change"
      ],
      "metadata": {
        "id": "YmhJncXcXH3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from leann.api import LeannChat\n",
        "\n",
        "if os.path.exists(INDEX_PATH):\n",
        "    llm_config = {\n",
        "        \"type\": \"hf\",\n",
        "        \"model\": \"Qwen/Qwen3-0.6B\",\n",
        "    }\n",
        "\n",
        "    chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)\n",
        "\n",
        "    # قم بتغيير السؤال أدناه ليناسب محتوى ملف الـ PDF الذي رفعته\n",
        "    question = \"what is Climate Change?\"\n",
        "\n",
        "    print(f\"طرح السؤال: {question}\")\n",
        "\n",
        "    response = chat.ask(\n",
        "        question,\n",
        "        top_k=3,  # استخدام 3 نتائج من البحث لتعزيز الإجابة\n",
        "        llm_kwargs={\"max_tokens\": 256},\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- الإجابة ---\")\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"ملف الفهرس غير موجود. الرجاء تشغيل خلية بناء الفهرس أولاً.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To7anYHIW94K",
        "outputId": "5ba7bc82-3acc-4bc1-d537-525df837cb40"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ملف الفهرس غير موجود. الرجاء تشغيل خلية بناء الفهرس أولاً.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mnDcJn8bYtKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zA0RokjFYtIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6WzJ61QwYtFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# الخطوة 1: تثبيت جميع المكتبات المطلوبة\n",
        "# ===================================================================\n",
        "print(\"--- الخطوة 1: جاري تثبيت المكتبات... ---\")\n",
        "# استخدام quiet=True لتقليل المخرجات غير الضرورية\n",
        "!pip install uv > /dev/null\n",
        "!uv pip install leann-core leann-backend-hnsw --no-deps --quiet\n",
        "!uv pip install leann --no-deps --quiet\n",
        "!pip install pypdf --quiet\n",
        "print(\"=> تم تثبيت المكتبات بنجاح.\\n\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 2: استيراد المكتبات والإعدادات الأولية\n",
        "# ===================================================================\n",
        "print(\"--- الخطوة 2: جاري إعداد البيئة... ---\")\n",
        "import os\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "from pypdf import PdfReader\n",
        "from leann.api import LeannBuilder, LeannSearcher, LeannChat\n",
        "\n",
        "# إعدادات لتسجيل معلومات أكثر تفصيلاً\n",
        "os.environ[\"LEANN_LOG_LEVEL\"] = \"INFO\"\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# تحديد مسار لحفظ الفهرس\n",
        "INDEX_DIR = Path(\"./\").resolve()\n",
        "INDEX_PATH = str(INDEX_DIR / \"demo.leann\")\n",
        "\n",
        "# متغيرات لتخزين اسم الملف والنص\n",
        "pdf_name = None\n",
        "text_chunks = []\n",
        "print(\"=> تم تجهيز الإعدادات بنجاح.\\n\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 3: رفع ملف الـ PDF\n",
        "# ===================================================================\n",
        "print(\"--- الخطوة 3: رفع ملف الـ PDF ---\")\n",
        "print(\"الرجاء اختيار ملف PDF من جهازك...\")\n",
        "try:\n",
        "    # حذف الفهرس القديم إذا كان موجودًا لضمان البدء من جديد\n",
        "    if os.path.exists(INDEX_PATH):\n",
        "        os.remove(INDEX_PATH)\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        pdf_name = next(iter(uploaded))\n",
        "        print(f\"\\n=> تم رفع الملف '{pdf_name}' بنجاح.\\n\")\n",
        "    else:\n",
        "        print(\"\\n!! لم يتم اختيار أي ملف. توقف التنفيذ. !!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n!! حدث خطأ أثناء رفع الملف: {e} !!\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 4: استخلاص النص من الـ PDF وتقسيمه\n",
        "# ===================================================================\n",
        "if pdf_name:\n",
        "    print(f\"--- الخطوة 4: جاري استخلاص النص من '{pdf_name}'... ---\")\n",
        "    try:\n",
        "        reader = PdfReader(pdf_name)\n",
        "        full_text = \"\"\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                full_text += page_text + \"\\n\"\n",
        "\n",
        "        # تقسيم النص الكامل إلى فقرات\n",
        "        text_chunks = [chunk.strip() for chunk in full_text.split('\\n\\n') if chunk.strip()]\n",
        "\n",
        "        if text_chunks:\n",
        "            print(f\"=> تم تقسيم النص إلى {len(text_chunks)} جزء/فقرة.\\n\")\n",
        "        else:\n",
        "            print(\"\\n!! لم يتم العثور على نص في ملف الـ PDF. قد يكون الملف عبارة عن صور. !!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!! حدث خطأ أثناء قراءة الملف: {e} !!\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 5: بناء الفهرس (Index)\n",
        "# ===================================================================\n",
        "if text_chunks:\n",
        "    print(\"--- الخطوة 5: جاري بناء الفهرس من محتوى الـ PDF... ---\")\n",
        "    builder = LeannBuilder(backend_name=\"hnsw\")\n",
        "    for chunk in text_chunks:\n",
        "        builder.add_text(chunk)\n",
        "    builder.build_index(INDEX_PATH)\n",
        "    print(f\"=> تم بناء الفهرس بنجاح وتم حفظه في: {INDEX_PATH}\\n\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 6: البحث في محتوى الملف\n",
        "# ===================================================================\n",
        "# التحقق من أن الفهرس تم إنشاؤه بنجاح\n",
        "if os.path.exists(INDEX_PATH):\n",
        "    print(\"--- الخطوة 6: تجربة البحث في المستند ---\")\n",
        "\n",
        "    # !!! هام: قم بتغيير النص أدناه للبحث عن أي شيء في المستند الخاص بك !!!\n",
        "    search_query = \"Climate Change\"\n",
        "\n",
        "    print(f\"جاري البحث عن: '{search_query}'...\")\n",
        "    searcher = LeannSearcher(INDEX_PATH)\n",
        "    results = searcher.search(search_query, top_k=2)\n",
        "\n",
        "    print(\"\\n--- نتائج البحث ---\")\n",
        "    if results:\n",
        "        for result in results:\n",
        "            print(f\"Score: {result.score:.4f}\")\n",
        "            print(f\"Text: {result.text}\\n\")\n",
        "    else:\n",
        "        print(\"لم يتم العثور على نتائج.\\n\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 7: الدردشة مع المستند (RAG)\n",
        "# ===================================================================\n",
        "# التحقق مرة أخرى قبل تشغيل الدردشة\n",
        "if os.path.exists(INDEX_PATH):\n",
        "    print(\"--- الخطوة 7: الدردشة مع المستند (RAG) ---\")\n",
        "    llm_config = {\n",
        "        \"type\": \"hf\",\n",
        "        \"model\": \"Qwen/Qwen3-0.6B\",\n",
        "    }\n",
        "    chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)\n",
        "\n",
        "    # !!! هام: قم بتغيير السؤال أدناه ليناسب محتوى ملف الـ PDF الذي رفعته !!!\n",
        "    question = \"what is Climate Change?\"\n",
        "\n",
        "    print(f\"طرح السؤال: {question}\\n\")\n",
        "\n",
        "    response = chat.ask(\n",
        "        question,\n",
        "        top_k=3,\n",
        "        llm_kwargs={\"max_tokens\": 256},\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- الإجابة النهائية من النموذج ---\")\n",
        "    print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3dcb69343a3e4f2580c644e0605c93f9",
            "5f4c6ef1c8bd43c090d53abde8ccf012",
            "e671e6f5bd184ea98638d481066e5644",
            "e573b2286ea2419e8696f3d7e7c38521",
            "d402a69bc58c45f9bc0a7bc06f8dd689",
            "3b7e1658076e469db2d3774d05ed3217",
            "ff4d4253d2944914bce7b0d0b8a40d31",
            "8b4a73dde6124308b5f6961a8bd5746d",
            "26a22c829fb7473695926925466eaeaf",
            "5fdb0f0594bd44e484195f19bc14adc0",
            "f30e5899459841c883a6ef378416cf01"
          ]
        },
        "id": "criQynhgYtCL",
        "outputId": "b8bab27b-49b4-42ff-dbb8-be7982a31b94"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- الخطوة 1: جاري تثبيت المكتبات... ---\n",
            "=> تم تثبيت المكتبات بنجاح.\n",
            "\n",
            "--- الخطوة 2: جاري إعداد البيئة... ---\n",
            "=> تم تجهيز الإعدادات بنجاح.\n",
            "\n",
            "--- الخطوة 3: رفع ملف الـ PDF ---\n",
            "الرجاء اختيار ملف PDF من جهازك...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-311cdd23-77e1-484f-bdb7-731308db313a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-311cdd23-77e1-484f-bdb7-731308db313a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Understanding_Climate_Change.pdf to Understanding_Climate_Change.pdf\n",
            "\n",
            "=> تم رفع الملف 'Understanding_Climate_Change.pdf' بنجاح.\n",
            "\n",
            "--- الخطوة 4: جاري استخلاص النص من 'Understanding_Climate_Change.pdf'... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'\n",
            "INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever\n",
            "INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 32, manual_tokenize=False)\n",
            "INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768\n",
            "INFO:leann.embedding_compute:Time taken: 0.06955289840698242 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> تم تقسيم النص إلى 1 جزء/فقرة.\n",
            "\n",
            "--- الخطوة 5: جاري بناء الفهرس من محتوى الـ PDF... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Writing passages: 100%|██████████| 1/1 [00:00<00:00, 767.63chunk/s]\n",
            "INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'\n",
            "INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever\n",
            "INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 32, manual_tokenize=False)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dcb69343a3e4f2580c644e0605c93f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768\n",
            "INFO:leann.embedding_compute:Time taken: 14.732553958892822 seconds\n",
            "WARNING:leann_backend_hnsw.hnsw_backend:Converting data to float32, shape: (1, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting conversion: /content/demo.index -> /content/demo.csr.tmp\n",
            "[0.00s] Reading Index HNSW header...\n",
            "[0.00s]   Header read: d=768, ntotal=1\n",
            "[0.00s] Reading HNSW struct vectors...\n",
            "  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48\n",
            "[0.00s]   Read assign_probas (6)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28\n",
            "[0.66s]   Read cum_nneighbor_per_level (7)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=1, Bytes=4\n",
            "[1.32s]   Read levels (1)\n",
            "[1.97s]   Probing for compact storage flag...\n",
            "[1.97s]   Found compact flag: False\n",
            "[1.97s]   Compact flag is False, reading original format...\n",
            "[1.97s]   Probing for potential extra byte before non-compact offsets...\n",
            "[1.97s]   Found and consumed an unexpected 0x00 byte.\n",
            "  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=2, Bytes=16\n",
            "[1.97s]   Read offsets (2)\n",
            "[2.78s]   Attempting to read neighbors vector...\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=64, Bytes=256\n",
            "[2.78s]   Read neighbors (64)\n",
            "[3.57s]   Read scalar params (ep=0, max_lvl=0)\n",
            "[3.57s] Checking for storage data...\n",
            "[3.57s]   Found storage fourcc: 49467849.\n",
            "[3.57s] Converting to CSR format...\n",
            "[3.57s]   Conversion loop finished.                        \n",
            "[3.57s] Running validation checks...\n",
            "    Checking total valid neighbor count...\n",
            "    OK: Total valid neighbors = 0\n",
            "    Checking final pointer indices...\n",
            "    OK: Final pointers match data size.\n",
            "[3.57s] Deleting original neighbors and offsets arrays...\n",
            "    CSR Stats: |data|=0, |level_ptr|=2\n",
            "[4.45s] Writing CSR HNSW graph data in FAISS-compatible order...\n",
            "   Pruning embeddings: Writing NULL storage marker.\n",
            "[5.16s] Conversion complete.\n",
            "=> تم بناء الفهرس بنجاح وتم حفظه في: /content/demo.leann\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install leann"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5BbH12mZ7Jc",
        "outputId": "d42eeb7a-296d-4402-a1e4-6b919e8da0b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m140 packages\u001b[0m \u001b[2min 3.15s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m24 packages\u001b[0m \u001b[2min 1.81s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 52ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m25 packages\u001b[0m \u001b[2min 97ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maiosqlite\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbanks\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcolorama\u001b[0m\u001b[2m==0.4.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdeprecated\u001b[0m\u001b[2m==1.2.18\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdirtyjson\u001b[0m\u001b[2m==1.0.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfiletype\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgitignore-parser\u001b[0m\u001b[2m==0.1.13\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgriffe\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mleann-backend-diskann\u001b[0m\u001b[2m==0.3.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mllama-index-core\u001b[0m\u001b[2m==0.14.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mllama-index-embeddings-huggingface\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mllama-index-instrumentation\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mllama-index-readers-file\u001b[0m\u001b[2m==0.5.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mllama-index-workflows\u001b[0m\u001b[2m==2.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpdfminer-six\u001b[0m\u001b[2m==20250506\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpdfplumber\u001b[0m\u001b[2m==0.11.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpymupdf\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpypdf2\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpypdfium2\u001b[0m\u001b[2m==4.30.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==75.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mstriprtf\u001b[0m\u001b[2m==0.0.26\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBrVcxTqZ6rM",
        "outputId": "1fab4ea8-5959-4803-ce48-b1e84e0a1ff5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "\n",
            "The smallest vector index in the world. RAG Everything with LEANN!\n",
            "\n",
            "positional arguments:\n",
            "  {build,search,ask,list,remove}\n",
            "                        Available commands\n",
            "    build               Build document index\n",
            "    search              Search documents\n",
            "    ask                 Ask questions\n",
            "    list                List all indexes\n",
            "    remove              Remove an index\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "Examples:\n",
            "  leann build my-docs --docs ./documents                                  # Build index from directory\n",
            "  leann build my-code --docs ./src ./tests ./config                      # Build index from multiple directories\n",
            "  leann build my-files --docs ./file1.py ./file2.txt ./docs/             # Build index from files and directories\n",
            "  leann build my-mixed --docs ./readme.md ./src/ ./config.json           # Build index from mixed files/dirs\n",
            "  leann build my-ppts --docs ./ --file-types .pptx,.pdf                  # Index only PowerPoint and PDF files\n",
            "  leann search my-docs \"query\"                                           # Search in my-docs index\n",
            "  leann ask my-docs \"question\"                                           # Ask my-docs index\n",
            "  leann list                                                             # List all stored indexes\n",
            "  leann remove my-docs                                                   # Remove an index (local first, then global)\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "===================================================================\n",
        "# الخطوة 1: تثبيت جميع المكتبات المطلوبة (بما في ذلك langchain)\n",
        "# ===================================================================\n",
        "print(\"--- الخطوة 1: جاري تثبيت المكتبات... ---\")\n",
        "!pip install uv > /dev/null\n",
        "!uv pip install leann-core leann-backend-hnsw --no-deps --quiet\n",
        "!uv pip install leann --no-deps --quiet\n",
        "!pip install pypdf langchain --quiet\n",
        "print(\"=> تم تثبيت المكتبات بنجاح.\\n\")"
      ],
      "metadata": {
        "id": "cfvt42DZaVsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 2: استيراد المكتبات والإعدادات الأولية\n",
        "# ===================================================================\n",
        "print(\"--- الخطوة 2: جاري إعداد البيئة... ---\")\n",
        "import os\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "from pypdf import PdfReader\n",
        "from leann.api import LeannBuilder, LeannSearcher, LeannChat\n",
        "# *** جديد: استيراد مقسم النصوص من langchain ***\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "os.environ[\"LEANN_LOG_LEVEL\"] = \"INFO\"\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "INDEX_DIR = Path(\"./\").resolve()\n",
        "INDEX_PATH = str(INDEX_DIR / \"demo.leann\")\n",
        "\n",
        "pdf_name = None\n",
        "text_chunks = []\n",
        "print(\"=> تم تجهيز الإعدادات بنجاح.\\n\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 3: رفع ملف الـ PDF\n",
        "# ===================================================================\n",
        "print(\"--- الخطوة 3: رفع ملف الـ PDF ---\")\n",
        "print(\"الرجاء اختيار ملف PDF من جهازك...\")\n",
        "try:\n",
        "    if os.path.exists(INDEX_PATH):\n",
        "        os.remove(INDEX_PATH)\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        pdf_name = next(iter(uploaded))\n",
        "        print(f\"\\n=> تم رفع الملف '{pdf_name}' بنجاح.\\n\")\n",
        "    else:\n",
        "        print(\"\\n!! لم يتم اختيار أي ملف. توقف التنفيذ. !!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n!! حدث خطأ أثناء رفع الملف: {e} !!\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 4: استخلاص النص وتقسيمه بطريقة احترافية\n",
        "# ===================================================================\n",
        "if pdf_name:\n",
        "    print(f\"--- الخطوة 4: جاري استخلاص وتقسيم النص من '{pdf_name}'... ---\")\n",
        "    try:\n",
        "        reader = PdfReader(pdf_name)\n",
        "        full_text = \"\"\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                full_text += page_text + \"\\n\"\n",
        "\n",
        "        # *** جديد: استخدام مقسم النصوص الاحترافي ***\n",
        "        # نقسم النص إلى أجزاء حجم كل منها 1000 حرف، مع تداخل 150 حرف لضمان عدم فقدان السياق\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=150,\n",
        "            length_function=len\n",
        "        )\n",
        "        text_chunks = text_splitter.split_text(full_text)\n",
        "\n",
        "        if text_chunks:\n",
        "            print(f\"=> تم تقسيم النص بنجاح إلى {len(text_chunks)} جزء.\\n\")\n",
        "        else:\n",
        "            print(\"\\n!! لم يتم العثور على نص في ملف الـ PDF. !!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!! حدث خطأ أثناء قراءة الملف: {e} !!\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 5: بناء الفهرس (Index)\n",
        "# ===================================================================\n",
        "if text_chunks:\n",
        "    print(\"--- الخطوة 5: جاري بناء الفهرس من محتوى الـ PDF... ---\")\n",
        "    builder = LeannBuilder(backend_name=\"hnsw\")\n",
        "    for chunk in text_chunks:\n",
        "        builder.add_text(chunk)\n",
        "    builder.build_index(INDEX_PATH)\n",
        "    print(f\"=> تم بناء الفهرس بنجاح وتم حفظه في: {INDEX_PATH}\\n\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 6: البحث في محتوى الملف\n",
        "# ===================================================================\n",
        "if os.path.exists(INDEX_PATH):\n",
        "    print(\"--- الخطوة 6: تجربة البحث في المستند ---\")\n",
        "    search_query = \"Climate Change\"\n",
        "    print(f\"جاري البحث عن: '{search_query}'...\")\n",
        "    searcher = LeannSearcher(INDEX_PATH)\n",
        "    results = searcher.search(search_query, top_k=3)\n",
        "\n",
        "    print(\"\\n--- أفضل 3 نتائج بحث ---\")\n",
        "    if results:\n",
        "        for i, result in enumerate(results):\n",
        "            print(f\"--- نتيجة {i+1} (Score: {result.score:.4f}) ---\")\n",
        "            print(f\"{result.text}\\n\")\n",
        "    else:\n",
        "        print(\"لم يتم العثور على نتائج.\\n\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 7: الدردشة مع المستند (RAG)\n",
        "# ===================================================================\n",
        "if os.path.exists(INDEX_PATH):\n",
        "    print(\"--- الخطوة 7: الدردشة مع المستند (RAG) ---\")\n",
        "    # *** رسالة توضيحية جديدة ***\n",
        "    print(\"سيتم الآن تحميل نموذج اللغة. قد تستغرق هذه العملية بضع دقائق في المرة الأولى، الرجاء الانتظار...\")\n",
        "\n",
        "    llm_config = {\n",
        "        \"type\": \"hf\",\n",
        "        \"model\": \"Qwen/Qwen3-0.6B\",\n",
        "    }\n",
        "    chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)\n",
        "\n",
        "    question = \"what is Climate Change based on the provided document?\"\n",
        "    print(f\"\\nطرح السؤال: {question}\\n\")\n",
        "\n",
        "    response = chat.ask(\n",
        "        question,\n",
        "        top_k=3,\n",
        "        llm_kwargs={\"max_tokens\": 512}, # زيادة الحد الأقصى للكلمات لإجابة أكثر تفصيلاً\n",
        "    )\n",
        "\n",
        "    print(\"\\n✅ --- الإجابة النهائية من النموذج --- ✅\")\n",
        "    print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f68b9577a6b24da8a0a1f74bd9243a47",
            "cd3a65f91ffa446a98b3a54f596d73a9",
            "50e5bf1eb4424c65b74fe4a296510988",
            "57183327e06e4118908a29c6ff12220a",
            "eaa9281f8b0d4a0aa4b9b24269bc2db4",
            "2eba1d81c5ec4343b452555e45e92cc5",
            "fd0a5e3d32ab44e880f5350c8634ece4",
            "babd63f76633418eb445d6c130870ede",
            "0ba6f13b721c486f9cc18a2ed3143b02",
            "39e0584951ff49d6a97cb8aec1ef51a6",
            "7fd909ab7893403cabb5ee84cf0b1032"
          ]
        },
        "id": "DvRLy4mjaGxb",
        "outputId": "ffc62e6f-c5f1-4729-a166-361fd905e2c2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- الخطوة 2: جاري إعداد البيئة... ---\n",
            "=> تم تجهيز الإعدادات بنجاح.\n",
            "\n",
            "--- الخطوة 3: رفع ملف الـ PDF ---\n",
            "الرجاء اختيار ملف PDF من جهازك...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-84ef49fb-9525-4ed9-8c74-b1eb0c421eea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-84ef49fb-9525-4ed9-8c74-b1eb0c421eea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Understanding_Climate_Change.pdf to Understanding_Climate_Change.pdf\n",
            "\n",
            "=> تم رفع الملف 'Understanding_Climate_Change.pdf' بنجاح.\n",
            "\n",
            "--- الخطوة 4: جاري استخلاص وتقسيم النص من 'Understanding_Climate_Change.pdf'... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'\n",
            "INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever\n",
            "INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 32, manual_tokenize=False)\n",
            "INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768\n",
            "INFO:leann.embedding_compute:Time taken: 0.0770103931427002 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> تم تقسيم النص بنجاح إلى 85 جزء.\n",
            "\n",
            "--- الخطوة 5: جاري بناء الفهرس من محتوى الـ PDF... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Writing passages: 100%|██████████| 85/85 [00:00<00:00, 16697.07chunk/s]\n",
            "INFO:leann.embedding_compute:Computing embeddings for 85 texts using SentenceTransformer, model: 'facebook/contriever'\n",
            "INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever\n",
            "INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 32, manual_tokenize=False)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f68b9577a6b24da8a0a1f74bd9243a47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:leann.embedding_compute:Generated 85 embeddings, dimension: 768\n",
            "INFO:leann.embedding_compute:Time taken: 422.77861738204956 seconds\n",
            "WARNING:leann_backend_hnsw.hnsw_backend:Converting data to float32, shape: (85, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting conversion: /content/demo.index -> /content/demo.csr.tmp\n",
            "[0.00s] Reading Index HNSW header...\n",
            "[0.00s]   Header read: d=768, ntotal=85\n",
            "[0.00s] Reading HNSW struct vectors...\n",
            "  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48\n",
            "[0.00s]   Read assign_probas (6)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28\n",
            "[0.68s]   Read cum_nneighbor_per_level (7)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=85, Bytes=340\n",
            "[1.35s]   Read levels (85)\n",
            "[2.01s]   Probing for compact storage flag...\n",
            "[2.01s]   Found compact flag: False\n",
            "[2.01s]   Compact flag is False, reading original format...\n",
            "[2.01s]   Probing for potential extra byte before non-compact offsets...\n",
            "[2.01s]   Found and consumed an unexpected 0x00 byte.\n",
            "  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=86, Bytes=688\n",
            "[2.01s]   Read offsets (86)\n",
            "[2.67s]   Attempting to read neighbors vector...\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=5504, Bytes=22016\n",
            "[2.67s]   Read neighbors (5504)\n",
            "[3.35s]   Read scalar params (ep=73, max_lvl=1)\n",
            "[3.35s] Checking for storage data...\n",
            "[3.35s]   Found storage fourcc: 49467849.\n",
            "[3.35s] Converting to CSR format...\n",
            "[3.35s]   Conversion loop finished.                        \n",
            "[3.35s] Running validation checks...\n",
            "    Checking total valid neighbor count...\n",
            "    OK: Total valid neighbors = 2672\n",
            "    Checking final pointer indices...\n",
            "    OK: Final pointers match data size.\n",
            "[3.35s] Deleting original neighbors and offsets arrays...\n",
            "    CSR Stats: |data|=2672, |level_ptr|=172\n",
            "[4.18s] Writing CSR HNSW graph data in FAISS-compatible order...\n",
            "   Pruning embeddings: Writing NULL storage marker.\n",
            "[4.98s] Conversion complete.\n",
            "=> تم بناء الفهرس بنجاح وتم حفظه في: /content/demo.leann\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(INDEX_PATH):\n",
        "    print(\"--- الخطوة 7: الدردشة مع المستند (RAG) ---\")\n",
        "    # *** رسالة توضيحية جديدة ***\n",
        "    print(\"سيتم الآن تحميل نموذج اللغة. قد تستغرق هذه العملية بضع دقائق في المرة الأولى، الرجاء الانتظار...\")\n",
        "\n",
        "    llm_config = {\n",
        "        \"type\": \"hf\",\n",
        "        \"model\": \"Qwen/Qwen3-0.6B\",\n",
        "    }\n",
        "    chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)\n",
        "\n",
        "    question = \"what is Climate Change based on the provided document?\"\n",
        "    print(f\"\\nطرح السؤال: {question}\\n\")\n",
        "\n",
        "    response = chat.ask(\n",
        "        question,\n",
        "        top_k=3,\n",
        "        llm_kwargs={\"max_tokens\": 512}, # زيادة الحد الأقصى للكلمات لإجابة أكثر تفصيلاً\n",
        "    )\n",
        "\n",
        "    print(\"\\n✅ --- الإجابة النهائية من النموذج --- ✅\")\n",
        "    print(response)"
      ],
      "metadata": {
        "id": "sL7FSS8ZcMkY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# الخطوة 1: تثبيت جميع المكتبات المطلوبة\n",
        "# ===================================================================\n",
        "print(\"--- الخطوة 1: جاري تثبيت المكتبات... ---\")\n",
        "!pip install uv > /dev/null\n",
        "!uv pip install leann-core leann-backend-hnsw --no-deps --quiet\n",
        "!uv pip install leann --no-deps --quiet\n",
        "!pip install pypdf langchain sentence-transformers --quiet\n",
        "print(\"=> تم تثبيت المكتبات بنجاح.\\n\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 2: استيراد المكتبات والإعدادات الأولية\n",
        "# ===================================================================\n",
        "print(\"--- الخطوة 2: جاري إعداد البيئة... ---\")\n",
        "import os\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "from pypdf import PdfReader\n",
        "from leann.api import LeannBuilder, LeannSearcher, LeannChat\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "os.environ[\"LEANN_LOG_LEVEL\"] = \"INFO\"\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "INDEX_DIR = Path(\"./\").resolve()\n",
        "INDEX_PATH = str(INDEX_DIR / \"demo.leann\")\n",
        "\n",
        "pdf_name = None\n",
        "text_chunks = []\n",
        "print(\"=> تم تجهيز الإعدادات بنجاح.\\n\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 3: رفع ملف الـ PDF\n",
        "# ===================================================================\n",
        "print(\"--- الخطوة 3: رفع ملف الـ PDF ---\")\n",
        "print(\"الرجاء اختيار ملف PDF من جهازك...\")\n",
        "try:\n",
        "    if os.path.exists(INDEX_PATH):\n",
        "        os.remove(INDEX_PATH)\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        pdf_name = next(iter(uploaded))\n",
        "        print(f\"\\n=> تم رفع الملف '{pdf_name}' بنجاح.\\n\")\n",
        "    else:\n",
        "        print(\"\\n!! لم يتم اختيار أي ملف. توقف التنفيذ. !!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n!! حدث خطأ أثناء رفع الملف: {e} !!\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 4: استخلاص النص وتقسيمه\n",
        "# ===================================================================\n",
        "if pdf_name:\n",
        "    print(f\"--- الخطوة 4: جاري استخلاص وتقسيم النص من '{pdf_name}'... ---\")\n",
        "    try:\n",
        "        reader = PdfReader(pdf_name)\n",
        "        full_text = \"\"\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                full_text += page_text + \"\\n\"\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=150,\n",
        "            length_function=len\n",
        "        )\n",
        "        text_chunks = text_splitter.split_text(full_text)\n",
        "\n",
        "        if text_chunks:\n",
        "            print(f\"=> تم تقسيم النص بنجاح إلى {len(text_chunks)} جزء.\\n\")\n",
        "        else:\n",
        "            print(\"\\n!! لم يتم العثور على نص في ملف الـ PDF. !!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!! حدث خطأ أثناء قراءة الملف: {e} !!\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 5: بناء الفهرس باستخدام نموذج خفيف\n",
        "# ===================================================================\n",
        "if text_chunks:\n",
        "    print(\"--- الخطوة 5: جاري بناء الفهرس باستخدام نموذج خفيف ومناسب للمعالج... ---\")\n",
        "\n",
        "    # *** التعديل الرئيسي: تحديد نموذج خفيف ومناسب للمعالج ***\n",
        "    builder = LeannBuilder(\n",
        "        backend_name=\"hnsw\",\n",
        "        embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    )\n",
        "\n",
        "    for chunk in text_chunks:\n",
        "        builder.add_text(chunk)\n",
        "    builder.build_index(INDEX_PATH)\n",
        "    print(f\"=> تم بناء الفهرس بنجاح وتم حفظه في: {INDEX_PATH}\\n\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 6: البحث في محتوى الملف\n",
        "# ===================================================================\n",
        "if os.path.exists(INDEX_PATH):\n",
        "    print(\"--- الخطوة 6: تجربة البحث في المستند ---\")\n",
        "    search_query = \"Climate Change\"\n",
        "    print(f\"جاري البحث عن: '{search_query}'...\")\n",
        "    searcher = LeannSearcher(INDEX_PATH)\n",
        "    results = searcher.search(search_query, top_k=3)\n",
        "\n",
        "    print(\"\\n--- أفضل 3 نتائج بحث ---\")\n",
        "    if results:\n",
        "        for i, result in enumerate(results):\n",
        "            print(f\"--- نتيجة {i+1} (Score: {result.score:.4f}) ---\")\n",
        "            print(f\"{result.text}\\n\")\n",
        "    else:\n",
        "        print(\"لم يتم العثور على نتائج.\\n\")\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# الخطوة 7: الدردشة مع المستند (RAG)\n",
        "# ===================================================================\n",
        "if os.path.exists(INDEX_PATH):\n",
        "    print(\"--- الخطوة 7: الدردشة مع المستند (RAG) ---\")\n",
        "    print(\"سيتم الآن تحميل نموذج اللغة. قد تستغرق هذه العملية بضع دقائق، الرجاء الانتظار...\")\n",
        "\n",
        "    llm_config = {\n",
        "        \"type\": \"hf\",\n",
        "        \"model\": \"Qwen/Qwen3-0.6B\",\n",
        "    }\n",
        "    chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)\n",
        "\n",
        "    question = \"what is Climate Change based on the provided document?\"\n",
        "    print(f\"\\nطرح السؤال: {question}\\n\")\n",
        "\n",
        "    response = chat.ask(\n",
        "        question,\n",
        "        top_k=3,\n",
        "        llm_kwargs={\"max_tokens\": 512},\n",
        "    )\n",
        "\n",
        "    print(\"\\n✅ --- الإجابة النهائية من النموذج --- ✅\")\n",
        "    print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6d7a4ca4ddd64bc0a5b162f21cb9c7f5",
            "6f709cedaf4940a68dbe3b9416a9d55d",
            "958fbb8b590d49898eb8540028cc8e39",
            "5ba7c2c5ccef4147b95aa493f40e1147",
            "ae1b6cc82bd84d2ebda5c6ad687223d3",
            "6b14c97988a64200821decdbcaf5ae2f",
            "ad1c6bb1f7f54181b979c4cd1166431a",
            "75a090289ace461b9ebc95ddbbf8a6b5",
            "2565dca9f04540c78d847e0e19af26bc",
            "fd900446bcd8435a8803f7cddcac8161",
            "933774f1714f4ed6b0daafc337182275"
          ]
        },
        "id": "0ByW_YiRhnoc",
        "outputId": "373cf671-4747-4bc4-e81b-857e371d64a9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- الخطوة 1: جاري تثبيت المكتبات... ---\n",
            "=> تم تثبيت المكتبات بنجاح.\n",
            "\n",
            "--- الخطوة 2: جاري إعداد البيئة... ---\n",
            "=> تم تجهيز الإعدادات بنجاح.\n",
            "\n",
            "--- الخطوة 3: رفع ملف الـ PDF ---\n",
            "الرجاء اختيار ملف PDF من جهازك...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-db85bd52-333b-4bae-bf9b-395c9666b68b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-db85bd52-333b-4bae-bf9b-395c9666b68b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Understanding_Climate_Change.pdf to Understanding_Climate_Change.pdf\n",
            "\n",
            "=> تم رفع الملف 'Understanding_Climate_Change.pdf' بنجاح.\n",
            "\n",
            "--- الخطوة 4: جاري استخلاص وتقسيم النص من 'Understanding_Climate_Change.pdf'... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'\n",
            "INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever\n",
            "INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 32, manual_tokenize=False)\n",
            "INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768\n",
            "INFO:leann.embedding_compute:Time taken: 0.07024145126342773 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> تم تقسيم النص بنجاح إلى 85 جزء.\n",
            "\n",
            "--- الخطوة 5: جاري بناء الفهرس باستخدام نموذج خفيف ومناسب للمعالج... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Writing passages: 100%|██████████| 85/85 [00:00<00:00, 17246.32chunk/s]\n",
            "INFO:leann.embedding_compute:Computing embeddings for 85 texts using SentenceTransformer, model: 'facebook/contriever'\n",
            "INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever\n",
            "INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 32, manual_tokenize=False)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d7a4ca4ddd64bc0a5b162f21cb9c7f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:leann.embedding_compute:Generated 85 embeddings, dimension: 768\n",
            "INFO:leann.embedding_compute:Time taken: 411.4871892929077 seconds\n",
            "WARNING:leann_backend_hnsw.hnsw_backend:Converting data to float32, shape: (85, 768)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting conversion: /content/leann/demo.index -> /content/leann/demo.csr.tmp\n",
            "[0.00s] Reading Index HNSW header...\n",
            "[0.00s]   Header read: d=768, ntotal=85\n",
            "[0.00s] Reading HNSW struct vectors...\n",
            "  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48\n",
            "[0.00s]   Read assign_probas (6)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28\n",
            "[0.87s]   Read cum_nneighbor_per_level (7)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=85, Bytes=340\n",
            "[1.54s]   Read levels (85)\n",
            "[2.21s]   Probing for compact storage flag...\n",
            "[2.21s]   Found compact flag: False\n",
            "[2.21s]   Compact flag is False, reading original format...\n",
            "[2.21s]   Probing for potential extra byte before non-compact offsets...\n",
            "[2.21s]   Found and consumed an unexpected 0x00 byte.\n",
            "  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=86, Bytes=688\n",
            "[2.21s]   Read offsets (86)\n",
            "[2.88s]   Attempting to read neighbors vector...\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=5504, Bytes=22016\n",
            "[2.88s]   Read neighbors (5504)\n",
            "[3.54s]   Read scalar params (ep=73, max_lvl=1)\n",
            "[3.54s] Checking for storage data...\n",
            "[3.54s]   Found storage fourcc: 49467849.\n",
            "[3.54s] Converting to CSR format...\n",
            "[3.55s]   Conversion loop finished.                        \n",
            "[3.55s] Running validation checks...\n",
            "    Checking total valid neighbor count...\n",
            "    OK: Total valid neighbors = 2672\n",
            "    Checking final pointer indices...\n",
            "    OK: Final pointers match data size.\n",
            "[3.55s] Deleting original neighbors and offsets arrays...\n",
            "    CSR Stats: |data|=2672, |level_ptr|=172\n",
            "[4.22s] Writing CSR HNSW graph data in FAISS-compatible order...\n",
            "   Pruning embeddings: Writing NULL storage marker.\n",
            "[4.88s] Conversion complete.\n",
            "=> تم بناء الفهرس بنجاح وتم حفظه في: /content/leann/demo.leann\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yichuan-w/LEANN.git leann\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhgwtOuzb4NN",
        "outputId": "bbfd70b9-4809-478c-e492-7b6bff706248"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'leann'...\n",
            "remote: Enumerating objects: 7157, done.\u001b[K\n",
            "remote: Counting objects: 100% (1281/1281), done.\u001b[K\n",
            "remote: Compressing objects: 100% (227/227), done.\u001b[K\n",
            "remote: Total 7157 (delta 1155), reused 1062 (delta 1054), pack-reused 5876 (from 2)\u001b[K\n",
            "Receiving objects: 100% (7157/7157), 70.87 MiB | 28.99 MiB/s, done.\n",
            "Resolving deltas: 100% (3957/3957), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd leann"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG12Zoqcb6Du",
        "outputId": "a26bab6d-1c90-4fd2-cbbb-6bc0b6e9cb3c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/leann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python examples"
      ],
      "metadata": {
        "id": "AUO_BFLeilxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!leann search /content/leann/Understanding_Climate_Change.pdf \"ما هي الفكرة الرئيسية في الفصل الثالث؟\" --top-k 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59IlXK4Ci_dl",
        "outputId": "6fdd31df-c645-4f8a-f503-6568f7f89fb6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/leann\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/leann/cli.py\", line 1567, in main\n",
            "    asyncio.run(cli.run())\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n",
            "    return future.result()\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/leann/cli.py\", line 1549, in run\n",
            "    await self.search_documents(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/leann/cli.py\", line 1359, in search_documents\n",
            "    all_matches = self._find_all_matching_indexes(index_name)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/leann/cli.py\", line 629, in _find_all_matching_indexes\n",
            "    for meta_file in project_path.rglob(f\"{index_name}.leann.meta.json\"):\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/pathlib.py\", line 1105, in rglob\n",
            "    raise NotImplementedError(\"Non-relative patterns are unsupported\")\n",
            "NotImplementedError: Non-relative patterns are unsupported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/leann/apps/base_rag_example.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAUQIQGtjoDE",
        "outputId": "a98d5046-30d3-43bc-fbd8-fb44deb024da"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/leann/apps/base_rag_example.py\", line 14, in <module>\n",
            "    from leann.settings import resolve_ollama_host, resolve_openai_api_key, resolve_openai_base_url\n",
            "ModuleNotFoundError: No module named 'leann.settings'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install leann"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbCHz3xek7Yp",
        "outputId": "1b398184-c428-4d98-8f92-18b952675e4c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 130ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQmLaxyhlAEu",
        "outputId": "87f0ea9d-eb5a-4136-fa24-62bd0a54e354"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[37m⠋\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m⠋\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mollama==0.6.0                                                                 \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mhttpx==0.28.1                                                                 \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mpydantic==2.11.9                                                              \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mpydantic-core==2.33.2                                                         \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2manyio==4.10.0                                                                 \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mcertifi==2025.8.3                                                             \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mhttpcore==1.0.9                                                               \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2midna==3.10                                                                    \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mannotated-types==0.7.0                                                        \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mtyping-extensions==4.15.0                                                     \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mtyping-extensions==4.15.0                                                     \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mtyping-inspection==0.4.1                                                      \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2msniffio==1.3.1                                                                \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mh11==0.16.0                                                                   \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2m                                                                              \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m13 packages\u001b[0m \u001b[2min 125ms\u001b[0m\u001b[0m\n",
            "\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/0)                                                   \r\u001b[2K\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2m\u001b[0m (1/1)                                                                        \r\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n",
            "░░░░░░░░░░░░░░░░░░░░ [0/0] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/1] \u001b[2mollama==0.6.0                                        \u001b[0m\r\u001b[2K████████████████████ [1/1] \u001b[2mollama==0.6.0                                        \u001b[0m\r\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mollama\u001b[0m\u001b[2m==0.6.0\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "nohup ollama serve &\n",
        "ollama pull llama3:8b\n",
        "ollama list"
      ],
      "metadata": {
        "id": "8aoK3xIYlJEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e8SNlP-lig1",
        "outputId": "e84463d0-da5c-4b36-d755-408fb0a7fbc4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "!ollama pull llama3.2:1b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo-B_JUAli9e",
        "outputId": "32a0ebe0-eefe-4da0-b378-bc21ad37d5f0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nAq9p3LimybV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m apps.document_rag --query \"What are the main techniques LEANN explores?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcWVFJHdmF7N",
        "outputId": "2a705d34-54fc-4b0c-c36f-aa437e131b42"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/leann/apps/document_rag.py\", line 12, in <module>\n",
            "    from base_rag_example import BaseRAGExample\n",
            "  File \"/content/leann/apps/base_rag_example.py\", line 14, in <module>\n",
            "    from leann.settings import resolve_ollama_host, resolve_openai_api_key, resolve_openai_base_url\n",
            "ModuleNotFoundError: No module named 'leann.settings'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/leann/apps/base_rag_example.py"
      ],
      "metadata": {
        "id": "raK0aUpJmzNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m apps.base_rag_example -help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64fJXI4fn8Fe",
        "outputId": "391cbb23-cbcb-4c22-ad10-096af900d6ac"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/leann/apps/base_rag_example.py\", line 14, in <module>\n",
            "    from leann.settings import resolve_ollama_host, resolve_openai_api_key, resolve_openai_base_url\n",
            "ModuleNotFoundError: No module named 'leann.settings'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m apps.base_rag_example --embedding-model sentence-transformers/all-mpnet-base-v2 --llm-model Qwen/Qwen3-0.6B  --query \"What are the main techniques LEANN explores?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ2oykCLm1NW",
        "outputId": "e9ffcbf8-bd88-4e05-88f1-16a7b232060f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/leann/apps/base_rag_example.py\", line 14, in <module>\n",
            "    from leann.settings import resolve_ollama_host, resolve_openai_api_key, resolve_openai_base_url\n",
            "ModuleNotFoundError: No module named 'leann.settings'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnF4MKSpoygO",
        "outputId": "5405a0f6-7e49-4f12-ee1a-604ab7b93e63"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME           ID              SIZE      MODIFIED       \n",
            "llama3.2:1b    baf6a787fdff    1.3 GB    10 minutes ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from leann import Client\n",
        "\n",
        "def build_index(index_name: str, docs_path: str):\n",
        "    \"\"\"\n",
        "    يبني الـ index من ملف (أو مجلد) المستندات.\n",
        "    \"\"\"\n",
        "    client = Client()\n",
        "    # إذا كان الـ index موجود يمكن استخدام force=True لإعادة البناء\n",
        "    client.build(index_name, docs=[docs_path], force=True)\n",
        "    print(f\"تم بناء الفهرس: {index_name}\")\n",
        "\n",
        "def ask_question(index_name: str, question: str, llm: str = \"openai\", model: str = \"gpt-4o-mini\", top_k: int = 5):\n",
        "    \"\"\"\n",
        "    يطرح سؤالًا على الـ index باستخدام LLM مختار.\n",
        "    \"\"\"\n",
        "    client = Client()\n",
        "    resp = client.ask(\n",
        "        index_name,\n",
        "        query=question,\n",
        "        llm=llm,\n",
        "        model=model,\n",
        "        top_k=top_k\n",
        "    )\n",
        "    return resp.answer, resp\n",
        "\n",
        "def main():\n",
        "    # --- الإعدادات (عدّلها حسب الكتاب وبيئتك) ---\n",
        "    index_name = \"my_book_index\"\n",
        "    book_path = \"book.pdf\"  # أو إذا استخدمت OCR، مثلاً \"book_ocr.pdf\"\n",
        "    # مفتاح OpenAI (إذا تستخدم واجهة OpenAI)\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"ضع_مفتاحك_هنا\"\n",
        "\n",
        "    # 1. بناء الفهرس\n",
        "    build_index(index_name, book_path)\n",
        "\n",
        "    # 2. طرح أسئلة\n",
        "    question = \"ما هي الفكرة الأساسية في الفصل الثالث؟\"\n",
        "    answer, full_resp = ask_question(index_name, question)\n",
        "    print(\"السؤال:\", question)\n",
        "    print(\"الإجابة:\", answer)\n",
        "\n",
        "    # يمكنك طرح أسئلة إضافية تكراريا\n",
        "    q2 = \"ماذا تعلمت عن موضوع X في الكتاب؟\"\n",
        "    ans2, resp2 = ask_question(index_name, q2)\n",
        "    print(\"السؤال:\", q2)\n",
        "    print(\"الإجابة:\", ans2)\n",
        "\n",
        "    # إذا تريد عرض التفاصيل (المقاطع المسترجعة، الأوزان، إلخ)\n",
        "    print(\"تفاصيل الاسترجاع:\", full_resp)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "DMSKY9pEopk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from leann import Client\n",
        "\n",
        "def build_index(index_name: str, docs_path: str):\n",
        "    client = Client()\n",
        "    # إذا كان الفهرس موجودًا فاستعمل force=True لإعادة البناء\n",
        "    client.build(index_name, docs=[docs_path], force=True)\n",
        "    print(f\"تم بناء الفهرس: {index_name}\")\n",
        "\n",
        "def ask_with_ollama(index_name: str, question: str, top_k: int = 5):\n",
        "    client = Client()\n",
        "    resp = client.ask(\n",
        "        index_name,\n",
        "        query=question,\n",
        "        llm=\"ollama\",\n",
        "        model=\"llama3.2:1b\",\n",
        "        top_k=top_k\n",
        "    )\n",
        "    return resp.answer, resp\n",
        "\n",
        "def main():\n",
        "    index_name = \"my_book_idx\"\n",
        "    book_path = \"/content/leann/Understanding_Climate_Change.pdf\"  # أو المسار إلى الكتاب الذي تريد استخدامه\n",
        "\n",
        "    # بناء الفهرس أولًا\n",
        "    build_index(index_name, book_path)\n",
        "\n",
        "    # الآن اسأل سؤالًا\n",
        "    q = \"What is the main idea in the book?\"\n",
        "    answer, resp = ask_with_ollama(index_name, q)\n",
        "    print(\"السؤال:\", q)\n",
        "    print(\"الإجابة:\", answer)\n",
        "\n",
        "    # إذا أردت مزيد من التفاصيل عن المقاطع المستخدمة\n",
        "    print(\"تفاصيل:\", resp)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "q3S8Jidbo9Fv",
        "outputId": "c8a88f4a-1317-48d2-a3b5-515a44bacf23"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Client' from 'leann' (/usr/local/lib/python3.12/dist-packages/leann/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2854880704.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mleann\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# إذا كان الفهرس موجودًا فاستعمل force=True لإعادة البناء\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Client' from 'leann' (/usr/local/lib/python3.12/dist-packages/leann/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m apps.document_rag --query \"What techniques does LEANN use?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXNFa8QjqBuO",
        "outputId": "15e137cc-e3e6-4bb8-d8b8-94d05908468f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/leann/apps/document_rag.py\", line 12, in <module>\n",
            "    from base_rag_example import BaseRAGExample\n",
            "  File \"/content/leann/apps/base_rag_example.py\", line 14, in <module>\n",
            "    from leann.settings import resolve_ollama_host, resolve_openai_api_key, resolve_openai_base_url\n",
            "ModuleNotFoundError: No module named 'leann.settings'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/leann"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP_R1Z4MqCVe",
        "outputId": "11c42652-2b4d-439e-afd3-ca031e7b21d1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/leann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m apps.document_rag --query \"What techniques does LEANN use?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FQUcFyAqVRe",
        "outputId": "be9e187c-6b35-486b-da51-e8619a9b3214"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/leann/apps/document_rag.py\", line 12, in <module>\n",
            "    from base_rag_example import BaseRAGExample\n",
            "  File \"/content/leann/apps/base_rag_example.py\", line 14, in <module>\n",
            "    from leann.settings import resolve_ollama_host, resolve_openai_api_key, resolve_openai_base_url\n",
            "ModuleNotFoundError: No module named 'leann.settings'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull nomic-embed-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJVFOYmNqW72",
        "outputId": "b06dc127-07b6-4d05-dae0-86fa3a0050b6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMji5vgQqg6P",
        "outputId": "40444937-b998-471f-b09a-a170fbb6135a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                       ID              SIZE      MODIFIED       \n",
            "nomic-embed-text:latest    0a109f422b47    274 MB    7 seconds ago     \n",
            "llama3.2:1b                baf6a787fdff    1.3 GB    18 minutes ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m apps.document_rag --query \"What techniques does LEANN use?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjSKFh4Pqkp2",
        "outputId": "2d5186cf-4a5f-4978-91cc-7592beb79471"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/leann/apps/document_rag.py\", line 12, in <module>\n",
            "    from base_rag_example import BaseRAGExample\n",
            "  File \"/content/leann/apps/base_rag_example.py\", line 14, in <module>\n",
            "    from leann.settings import resolve_ollama_host, resolve_openai_api_key, resolve_openai_base_url\n",
            "ModuleNotFoundError: No module named 'leann.settings'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export LEANN_OLLAMA_HOST=\"http://localhost:11434\""
      ],
      "metadata": {
        "id": "B48a1yH6qmxl"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/Understanding_Climate_Change.pdf \\\n",
        "  --llm ollama \\\n",
        "  --llm-model llama3.2 \\\n",
        "  --host http://192.168.1.101:11434"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VexQUlMoqq82",
        "outputId": "cd036c2a-3316-4ee7-8491-e38e4bd299c0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --llm-model llama3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from leann.api import LeannBuilder\n",
        "\n",
        "builder = LeannBuilder(\n",
        "    backend_name=\"hnsw\",\n",
        "    embedding_model=\"nomic-embed-text:latest\",\n",
        "    embedding_options={\n",
        "        \"base_url\": \"http://192.168.1.101:11434/v1\",\n",
        "    },\n",
        ")\n",
        "builder.build_index(\"/content/leann\", chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "19oXPAgFrF6W",
        "outputId": "6938cc97-dc85-4d39-de69-77e236807de5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chunks' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3121492048.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     },\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/leann\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python apps/document_rag.py --query \"What are the main techniques LEANN explores?\" \\\n",
        "  --index-dir hnswbuild --backend hnsw \\\n",
        "  --llm ollama --llm-model llama3.2:1b --thinking-budget high"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHGBz1jwsEdX",
        "outputId": "93699fc7-9cd4-4164-bac1-3d3477862d44"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/leann/apps/document_rag.py\", line 12, in <module>\n",
            "    from base_rag_example import BaseRAGExample\n",
            "  File \"/content/leann/apps/base_rag_example.py\", line 14, in <module>\n",
            "    from leann.settings import resolve_ollama_host, resolve_openai_api_key, resolve_openai_base_url\n",
            "ModuleNotFoundError: No module named 'leann.settings'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/yichuan-w/LEANN.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tf6FW7tXtE6v",
        "outputId": "d70fd283-3580-422d-eff0-9c98d5e1293a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/yichuan-w/LEANN.git\n",
            "  Cloning https://github.com/yichuan-w/LEANN.git to /tmp/pip-req-build-6nyr9oq_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/yichuan-w/LEANN.git /tmp/pip-req-build-6nyr9oq_\n",
            "  Resolved https://github.com/yichuan-w/LEANN.git to commit fecee94af11d1751b5c6951d4732b5a47bed6a8a\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: leann-core in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (0.3.4)\n",
            "Requirement already satisfied: leann-backend-hnsw in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (0.3.4)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (0.17.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: datasets>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (4.0.0)\n",
            "Collecting evaluate (from leann-workspace==0.1.0)\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (0.4.6)\n",
            "Collecting boto3 (from leann-workspace==0.1.0)\n",
            "  Downloading boto3-1.40.38-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting protobuf==4.25.3 (from leann-workspace==0.1.0)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sglang (from leann-workspace==0.1.0)\n",
            "  Downloading sglang-0.5.2-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (0.6.0)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (5.1.0)\n",
            "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (1.108.0)\n",
            "Requirement already satisfied: PyPDF2>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (3.0.1)\n",
            "Requirement already satisfied: pdfplumber>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (0.11.7)\n",
            "Requirement already satisfied: pymupdf>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: pypdfium2>=4.30.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (4.30.0)\n",
            "Collecting llama-index>=0.12.44 (from leann-workspace==0.1.0)\n",
            "  Downloading llama_index-0.14.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: llama-index-readers-file>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (0.5.4)\n",
            "Collecting llama-index-vector-stores-faiss>=0.4.0 (from leann-workspace==0.1.0)\n",
            "  Downloading llama_index_vector_stores_faiss-0.5.1-py3-none-any.whl.metadata (377 bytes)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface>=0.5.5 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (0.6.1)\n",
            "Collecting ipykernel==6.29.5 (from leann-workspace==0.1.0)\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: msgpack>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (1.1.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (5.9.5)\n",
            "Collecting pybind11>=3.0.0 (from leann-workspace==0.1.0)\n",
            "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting pathspec>=0.12.1 (from leann-workspace==0.1.0)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: nbconvert>=7.16.6 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (7.16.6)\n",
            "Requirement already satisfied: gitignore-parser>=0.1.12 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (0.1.13)\n",
            "Collecting astchunk>=0.1.0 (from leann-workspace==0.1.0)\n",
            "  Downloading astchunk-0.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tree-sitter>=0.20.0 (from leann-workspace==0.1.0)\n",
            "  Downloading tree_sitter-0.25.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10.0 kB)\n",
            "Collecting tree-sitter-python>=0.20.0 (from leann-workspace==0.1.0)\n",
            "  Downloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tree-sitter-java>=0.20.0 (from leann-workspace==0.1.0)\n",
            "  Downloading tree_sitter_java-0.23.5-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting tree-sitter-c-sharp>=0.20.0 (from leann-workspace==0.1.0)\n",
            "  Downloading tree_sitter_c_sharp-0.23.1-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting tree-sitter-typescript>=0.20.0 (from leann-workspace==0.1.0)\n",
            "  Downloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: torchvision>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from leann-workspace==0.1.0) (0.23.0+cu126)\n",
            "Collecting comm>=0.1.1 (from ipykernel==6.29.5->leann-workspace==0.1.0)\n",
            "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.12/dist-packages (from ipykernel==6.29.5->leann-workspace==0.1.0) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel==6.29.5->leann-workspace==0.1.0) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel==6.29.5->leann-workspace==0.1.0) (7.4.9)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel==6.29.5->leann-workspace==0.1.0) (5.8.1)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel==6.29.5->leann-workspace==0.1.0) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel==6.29.5->leann-workspace==0.1.0) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ipykernel==6.29.5->leann-workspace==0.1.0) (25.0)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.12/dist-packages (from ipykernel==6.29.5->leann-workspace==0.1.0) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel==6.29.5->leann-workspace==0.1.0) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel==6.29.5->leann-workspace==0.1.0) (5.7.1)\n",
            "Collecting pyrsistent>=0.18.0 (from astchunk>=0.1.0->leann-workspace==0.1.0)\n",
            "  Downloading pyrsistent-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.15.0->leann-workspace==0.1.0) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.15.0->leann-workspace==0.1.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.15.0->leann-workspace==0.1.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.15.0->leann-workspace==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.15.0->leann-workspace==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.15.0->leann-workspace==0.1.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15.0->leann-workspace==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.15.0->leann-workspace==0.1.0) (0.35.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.15.0->leann-workspace==0.1.0) (6.0.2)\n",
            "Collecting llama-index-cli<0.6,>=0.5.0 (from llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_index_cli-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: llama-index-core<0.15,>=0.14.3 in /usr/local/lib/python3.12/dist-packages (from llama-index>=0.12.44->leann-workspace==0.1.0) (0.14.3)\n",
            "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-index-llms-openai<0.6,>=0.5.0 (from llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_index_llms_openai-0.5.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index>=0.12.44->leann-workspace==0.1.0) (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file>=0.4.0->leann-workspace==0.1.0) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file>=0.4.0->leann-workspace==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: pypdf<7,>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file>=0.4.0->leann-workspace==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file>=0.4.0->leann-workspace==0.1.0) (0.0.26)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=7.16.6->leann-workspace==0.1.0) (6.2.0)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=7.16.6->leann-workspace==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert>=7.16.6->leann-workspace==0.1.0) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=7.16.6->leann-workspace==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=7.16.6->leann-workspace==0.1.0) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=7.16.6->leann-workspace==0.1.0) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=7.16.6->leann-workspace==0.1.0) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=7.16.6->leann-workspace==0.1.0) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=7.16.6->leann-workspace==0.1.0) (2.19.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->leann-workspace==0.1.0) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->leann-workspace==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->leann-workspace==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->leann-workspace==0.1.0) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->leann-workspace==0.1.0) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->leann-workspace==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0->leann-workspace==0.1.0) (4.15.0)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.12/dist-packages (from pdfplumber>=0.11.0->leann-workspace==0.1.0) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber>=0.11.0->leann-workspace==0.1.0) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber>=0.11.0->leann-workspace==0.1.0) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber>=0.11.0->leann-workspace==0.1.0) (43.0.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->leann-workspace==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->leann-workspace==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->leann-workspace==0.1.0) (2025.8.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.2.0->leann-workspace==0.1.0) (4.56.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.2.0->leann-workspace==0.1.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.2.0->leann-workspace==0.1.0) (1.16.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->leann-workspace==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.12.3->leann-workspace==0.1.0) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.12.3->leann-workspace==0.1.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.12.3->leann-workspace==0.1.0) (13.9.4)\n",
            "Collecting botocore<1.41.0,>=1.40.38 (from boto3->leann-workspace==0.1.0)\n",
            "  Downloading botocore-1.40.38-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->leann-workspace==0.1.0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3->leann-workspace==0.1.0)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from leann-core->leann-workspace==0.1.0) (1.1.1)\n",
            "Requirement already satisfied: accelerate>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from leann-core->leann-workspace==0.1.0) (1.10.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from sglang->leann-workspace==0.1.0) (3.12.15)\n",
            "Collecting setproctitle (from sglang->leann-workspace==0.1.0)\n",
            "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.20.0->leann-core->leann-workspace==0.1.0) (0.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file>=0.4.0->leann-workspace==0.1.0) (2.8)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=7.16.6->leann-workspace==0.1.0) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=7.16.6->leann-workspace==0.1.0) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.38->boto3->leann-workspace==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->sglang->leann-workspace==0.1.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->sglang->leann-workspace==0.1.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->sglang->leann-workspace==0.1.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->sglang->leann-workspace==0.1.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->sglang->leann-workspace==0.1.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->sglang->leann-workspace==0.1.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->sglang->leann-workspace==0.1.0) (1.20.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->leann-workspace==0.1.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->leann-workspace==0.1.0) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.15.0->leann-workspace==0.1.0) (1.1.10)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel==6.29.5->leann-workspace==0.1.0)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel==6.29.5->leann-workspace==0.1.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel==6.29.5->leann-workspace==0.1.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel==6.29.5->leann-workspace==0.1.0) (3.0.52)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel==6.29.5->leann-workspace==0.1.0) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel==6.29.5->leann-workspace==0.1.0) (4.9.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel==6.29.5->leann-workspace==0.1.0) (0.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.29.5->leann-workspace==0.1.0) (4.4.0)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (1.2.0)\n",
            "Requirement already satisfied: llama-index-workflows<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (0.11.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (1.17.3)\n",
            "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.69-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert>=7.16.6->leann-workspace==0.1.0) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert>=7.16.6->leann-workspace==0.1.0) (4.25.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index>=0.12.44->leann-workspace==0.1.0) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index>=0.12.44->leann-workspace==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.15.0->leann-workspace==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.15.0->leann-workspace==0.1.0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->leann-workspace==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->leann-workspace==0.1.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->leann-workspace==0.1.0) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.12.3->leann-workspace==0.1.0) (4.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->leann-workspace==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0->leann-workspace==0.1.0) (0.22.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.2.0->leann-workspace==0.1.0) (3.6.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (1.14.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber>=0.11.0->leann-workspace==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel==6.29.5->leann-workspace==0.1.0) (0.8.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=7.16.6->leann-workspace==0.1.0) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=7.16.6->leann-workspace==0.1.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert>=7.16.6->leann-workspace==0.1.0) (0.27.1)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (0.4.1)\n",
            "Collecting llama-cloud-services>=0.6.69 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.69-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->leann-workspace==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel==6.29.5->leann-workspace==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel==6.29.5->leann-workspace==0.1.0) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.38->boto3->leann-workspace==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.14.3->llama-index>=0.12.44->leann-workspace==0.1.0) (3.26.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber>=0.11.0->leann-workspace==0.1.0) (2.23)\n",
            "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.68-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.68 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.68-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.67-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.67 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.67-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.66-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.66 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.66-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.65-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting llama-cloud-services>=0.6.64 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.65-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading llama_cloud_services-0.6.64-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.64-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.63-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.63 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.63-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index>=0.12.44->leann-workspace==0.1.0)\n",
            "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astchunk-0.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading llama_index-0.14.3-py3-none-any.whl (7.4 kB)\n",
            "Downloading llama_index_vector_stores_faiss-0.5.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter-0.25.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (634 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.6/634.6 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_c_sharp-0.23.1-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (402 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_java-0.23.5-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.38-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sglang-0.5.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.38-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading llama_index_cli-0.5.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl (17 kB)\n",
            "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.5.6-py3-none-any.whl (25 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl (3.2 kB)\n",
            "Downloading pyrsistent-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (122 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.3/122.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: leann-workspace\n",
            "  Building wheel for leann-workspace (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for leann-workspace: filename=leann_workspace-0.1.0-py3-none-any.whl size=4734 sha256=9e975daf22700e275182c91ce511d6117817198d813132150a004f2fc9c9ad87\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4wczc_63/wheels/99/24/ab/586b8ce1ba75e4da0f0ce9a066a77c8ee9857f7677d35dc3f2\n",
            "Successfully built leann-workspace\n",
            "Installing collected packages: tree-sitter-typescript, tree-sitter-python, tree-sitter-java, tree-sitter-c-sharp, tree-sitter, setproctitle, pyrsistent, pybind11, protobuf, pathspec, jmespath, jedi, comm, botocore, astchunk, sglang, s3transfer, llama-cloud, ipykernel, boto3, evaluate, llama-index-vector-stores-faiss, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-cli, llama-index-readers-llama-parse, llama-index, leann-workspace\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 6.17.1\n",
            "    Uninstalling ipykernel-6.17.1:\n",
            "      Successfully uninstalled ipykernel-6.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==6.17.1, but you have ipykernel 6.29.5 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed astchunk-0.1.0 boto3-1.40.38 botocore-1.40.38 comm-0.2.3 evaluate-0.4.6 ipykernel-6.29.5 jedi-0.19.2 jmespath-1.0.1 leann-workspace-0.1.0 llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-0.14.3 llama-index-cli-0.5.1 llama-index-embeddings-openai-0.5.1 llama-index-indices-managed-llama-cloud-0.9.4 llama-index-llms-openai-0.5.6 llama-index-readers-llama-parse-0.5.1 llama-index-vector-stores-faiss-0.5.1 llama-parse-0.6.54 pathspec-0.12.1 protobuf-4.25.3 pybind11-3.0.1 pyrsistent-0.20.0 s3transfer-0.14.0 setproctitle-1.3.7 sglang-0.5.2 tree-sitter-0.25.1 tree-sitter-c-sharp-0.23.1 tree-sitter-java-0.23.5 tree-sitter-python-0.25.0 tree-sitter-typescript-0.23.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "ipykernel"
                ]
              },
              "id": "d3df4f02881d444caab755823237794e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/leann"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MX-I4vDtX4v",
        "outputId": "0816dcdd-e710-4ab3-da24-8aeb0326f31b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/leann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann build \\\n",
        "  --add-file \"/content/leann/Understanding_Climate_Change.pdf\" \\\n",
        "  --index-path \"./my_book.leann\" \\\n",
        "  --backend \"hnsw\" \\\n",
        "  --embedding-model-name \"sentence-transformers/all-MiniLM-L6-v2\" \\\n",
        "  --chunk-size 1000 \\\n",
        "  --chunk-overlap 150"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roria7lVtFYf",
        "outputId": "d552efb1-f10a-4f8e-87f3-7fe26f0fd3c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --add-file --index-path ./my_book.leann --embedding-model-name sentence-transformers/all-MiniLM-L6-v2 --chunk-size 1000 --chunk-overlap 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann build leann [-h] {build,search,ask,list,remove}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxRaE18Wtj_Q",
        "outputId": "567acf12-ec06-4676-cb58-1345d1a38f59"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: [-h] build search ask list remove\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkP3tgqCtnfm",
        "outputId": "79ef547c-72d0-483b-8a8d-d4aac2594276"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "\n",
            "The smallest vector index in the world. RAG Everything with LEANN!\n",
            "\n",
            "positional arguments:\n",
            "  {build,search,ask,list,remove}\n",
            "                        Available commands\n",
            "    build               Build document index\n",
            "    search              Search documents\n",
            "    ask                 Ask questions\n",
            "    list                List all indexes\n",
            "    remove              Remove an index\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "Examples:\n",
            "  leann build my-docs --docs ./documents                                  # Build index from directory\n",
            "  leann build my-code --docs ./src ./tests ./config                      # Build index from multiple directories\n",
            "  leann build my-files --docs ./file1.py ./file2.txt ./docs/             # Build index from files and directories\n",
            "  leann build my-mixed --docs ./readme.md ./src/ ./config.json           # Build index from mixed files/dirs\n",
            "  leann build my-ppts --docs ./ --file-types .pptx,.pdf                  # Index only PowerPoint and PDF files\n",
            "  leann search my-docs \"query\"                                           # Search in my-docs index\n",
            "  leann ask my-docs \"question\"                                           # Ask my-docs index\n",
            "  leann list                                                             # List all stored indexes\n",
            "  leann remove my-docs                                                   # Remove an index (local first, then global)\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "usage: leann [-h] {build,search,ask,list,remove} ...\n",
        "\n",
        "The smallest vector index in the world. RAG Everything with LEANN!\n",
        "\n",
        "positional arguments:\n",
        "  {build,search,ask,list,remove}\n",
        "                        Available commands\n",
        "    build               Build document index\n",
        "    search              Search documents\n",
        "    ask                 Ask questions\n",
        "    list                List all indexes\n",
        "    remove              Remove an index\n",
        "\n",
        "options:\n",
        "  -h, --help            show this help message and exit\n",
        "\n",
        "Examples:\n",
        "  leann build my-docs --docs ./documents                                  # Build index from directory\n",
        "  leann build my-code --docs ./src ./tests ./config                      # Build index from multiple directories\n",
        "  leann build my-files --docs ./file1.py ./file2.txt ./docs/             # Build index from files and directories\n",
        "  leann build my-mixed --docs ./readme.md ./src/ ./config.json           # Build index from mixed files/dirs\n",
        "  leann build my-ppts --docs ./ --file-types .pptx,.pdf                  # Index only PowerPoint and PDF files\n",
        "  leann search my-docs \"query\"                                           # Search in my-docs index\n",
        "  leann ask my-docs \"question\"                                           # Ask my-docs index\n",
        "  leann list                                                             # List all stored indexes\n",
        "  leann remove my-docs   "
      ],
      "metadata": {
        "id": "67z8TqTut81v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغاله"
      ],
      "metadata": {
        "id": "0BK4IX7M9t-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!leann build my-docs --docs ./data --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSYJoUK7tr4-",
        "outputId": "022df64f-85c0-4fd1-aaa0-7bcb61ee790d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Indexing 1 path:\n",
            "  📁 Directories (1):\n",
            "    1. /content/leann/data\n",
            "Loading documents from 1 directory (1 total):\n",
            "  📁 Directories: data\n",
            "\n",
            "🔄 Processing 1 directory...\n",
            "Processing directory: data\n",
            "📋 No .gitignore found\n",
            "Loading files: 100% 1/1 [00:00<00:00,  2.74it/s]\n",
            "Loaded 1 documents from data\n",
            "start chunking documents\n",
            "Chunking documents: 100% 1/1 [00:00<00:00, 852.33doc/s]\n",
            "Loaded 1 documents, 1 chunks\n",
            "Building index 'my-docs' with hnsw backend...\n",
            "2025-09-25 02:53:53.258429: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758768833.283103   31229 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758768833.290696   31229 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758768833.310059   31229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758768833.310101   31229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758768833.310106   31229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758768833.310112   31229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-25 02:53:53.315576: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Writing passages: 100% 1/1 [00:00<00:00, 11586.48chunk/s]\n",
            "Batches: 100% 1/1 [00:00<00:00,  1.85it/s]\n",
            "WARNING:leann_backend_hnsw.hnsw_backend:Converting data to float32, shape: (1, 768)\n",
            "M: 64 for level: 0\n",
            "Starting conversion: /content/leann/.leann/indexes/my-docs/documents.index -> /content/leann/.leann/indexes/my-docs/documents.csr.tmp\n",
            "[0.00s] Reading Index HNSW header...\n",
            "[0.00s]   Header read: d=768, ntotal=1\n",
            "[0.00s] Reading HNSW struct vectors...\n",
            "  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48\n",
            "[0.00s]   Read assign_probas (6)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28\n",
            "[0.48s]   Read cum_nneighbor_per_level (7)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=1, Bytes=4\n",
            "[0.95s]   Read levels (1)\n",
            "[1.55s]   Probing for compact storage flag...\n",
            "[1.55s]   Found compact flag: False\n",
            "[1.55s]   Compact flag is False, reading original format...\n",
            "[1.55s]   Probing for potential extra byte before non-compact offsets...\n",
            "[1.55s]   Found and consumed an unexpected 0x00 byte.\n",
            "  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=2, Bytes=16\n",
            "[1.55s]   Read offsets (2)\n",
            "[2.14s]   Attempting to read neighbors vector...\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=64, Bytes=256\n",
            "[2.14s]   Read neighbors (64)\n",
            "[2.74s]   Read scalar params (ep=0, max_lvl=0)\n",
            "[2.74s] Checking for storage data...\n",
            "[2.74s]   Found storage fourcc: 49467849.\n",
            "[2.74s] Converting to CSR format...\n",
            "[2.74s]   Conversion loop finished.                        \n",
            "[2.74s] Running validation checks...\n",
            "    Checking total valid neighbor count...\n",
            "    OK: Total valid neighbors = 0\n",
            "    Checking final pointer indices...\n",
            "    OK: Final pointers match data size.\n",
            "[2.74s] Deleting original neighbors and offsets arrays...\n",
            "    CSR Stats: |data|=0, |level_ptr|=2\n",
            "[3.21s] Writing CSR HNSW graph data in FAISS-compatible order...\n",
            "   Pruning embeddings: Writing NULL storage marker.\n",
            "[3.63s] Conversion complete.\n",
            "Index built at /content/leann/.leann/indexes/my-docs/documents.leann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/leann/.leann/indexes/my-docs"
      ],
      "metadata": {
        "id": "GYt73EIpxo0Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mXAEuxIPxqRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!leann build my-ppts --docs ./ --file-types .pptx,.pdf"
      ],
      "metadata": {
        "id": "KuSXJ-NCuJ0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! leann ask my-docs \"What is the author's name as it appears in the context?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuB0SjEdugDG",
        "outputId": "4fa86b88-8d6a-4c96-c06b-062ea110a384"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: What is the author's name as it appears in the context?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغالة"
      ],
      "metadata": {
        "id": "yGhWJhQ_9ZXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!leann search my-docs \"author\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMA6nCR0uf_-",
        "outputId": "043a9bf4-a081-418b-843c-9f5d5fb3946f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
            "[read_HNSW NL v4] Read levels vector, size: 1\n",
            "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
            "[read_HNSW NL v4] Read compact_level_ptr, size: 2\n",
            "[read_HNSW NL v4] Read compact_node_offsets, size: 2\n",
            "[read_HNSW NL v4] Read entry_point: 0, max_level: 0\n",
            "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
            "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 214\n",
            "[read_HNSW NL v4] Reading neighbors data into memory.\n",
            "[read_HNSW NL v4] Read neighbors data, size: 0\n",
            "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
            "INFO: Skipping external storage loading, since is_recompute is true.\n",
            "WARNING:leann.api:  ⚠️  Requested top_k (5) exceeds total documents (1)\n",
            "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
            "ZmqDistanceComputer initialized: d=768, metric=0\n",
            "Search results for 'author' (top 1):\n",
            "1. Score: 0.541\n",
            "   Title: Pride and Prejudice\n",
            "\n",
            "Author: Jane Austen\n",
            "\n",
            "Release date: June 1, 1998 [eBook #1342]\n",
            "                Most recently updated: October 29, 2024\n",
            "\n",
            "Language: English...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python apps/document_rag.py --query \"What are the main techniques LEANN explores?\" \\\n",
        "  --index-dir hnswbuild --backend hnsw \\\n",
        "  --llm ollama --llm-model gpt-oss:20b --thinking-budget high"
      ],
      "metadata": {
        "id": "93LA7wdcuf5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!leann build \\\n",
        "  --add-file \"/path/to/your/book.pdf\" \\\n",
        "  --index-path \"./my_book.leann\" \\\n",
        "  --backend \"hnsw\" \\\n",
        "  --embedding-model-name \"sentence-transformers/all-MiniLM-L6-v2\" \\\n",
        "  --chunk-size 1000 \\\n",
        "  --chunk-overlap 150"
      ],
      "metadata": {
        "id": "nrnG4-_Euf13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask \"What is the main theme of the story?\" \\\n",
        "  --index-path \"./my_book.leann\" \\\n",
        "  --llm-type \"hf\" \\\n",
        "  --llm-model \"Qwen/Qwen3-0.6B\" \\\n",
        "  --top-k 3"
      ],
      "metadata": {
        "id": "ib4TEmKAuid-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7MeTxD_fuzTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61fG0KWiuzMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!leann build my-book-index \\\n",
        "  --docs \"/content/leann/Understanding_Climate_Change.pdf\" \\\n",
        "  --embedding-model-name \"sentence-transformers/all-MiniLM-L6-v2\" \\\n",
        "  --chunk-size 1000 \\\n",
        "  --chunk-overlap 150"
      ],
      "metadata": {
        "id": "BY6JKBV9uobS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs \"What is the author's name as it appears in the context?\" \\\n",
        "  --llm-model \"Qwen/Qwen3-0.6B\" \\\n",
        "  --top-k 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es9Qqmv0uqy3",
        "outputId": "6f71410e-8aca-4c73-8b70-e11ce6855ba1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: What is the author's name as it appears in the context? --llm-model Qwen/Qwen3-0.6B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!leann ask my-docs \"What is the author's name as it appears in the context?\" \\\n",
        "  --llm-model \"Qwen/Qwen3-0.6B\" \\\n",
        "  --embedding-mode ollama --embedding-model nomic-embed-text \\\n",
        "  --top-k 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiH0maiN3a1C",
        "outputId": "1211256f-5d15-468e-fab2-c42c3f16e16e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['usage: leann [-h] {build,search,ask,list,remove} ...',\n",
              " \"leann: error: unrecognized arguments: What is the author's name as it appears in the context? --llm-model Qwen/Qwen3-0.6B --embedding-mode ollama --embedding-model nomic-embed-text\"]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m apps.document_rag --query \"What techniques does LEANN use?\" --llm ollama --llm-model gpt-oss:20b"
      ],
      "metadata": {
        "id": "KJngLQn54Lap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask my-docs \"author\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e6F2yGn3ry0",
        "outputId": "c79d8e2b-143f-4592-b258-4e7b908c4759"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: author\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann [-h] {build,search,ask,list,remove}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnxwMzPqzsog",
        "outputId": "11d7a2bc-ab40-4dce-ed8b-fdd35ed99e56"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: argument command: invalid choice: '[-h]' (choose from build, search, ask, list, remove)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann -h ask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQNsBjBFz3n0",
        "outputId": "4f375689-7134-4dcb-8058-8fd288a935e0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "\n",
            "The smallest vector index in the world. RAG Everything with LEANN!\n",
            "\n",
            "positional arguments:\n",
            "  {build,search,ask,list,remove}\n",
            "                        Available commands\n",
            "    build               Build document index\n",
            "    search              Search documents\n",
            "    ask                 Ask questions\n",
            "    list                List all indexes\n",
            "    remove              Remove an index\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "Examples:\n",
            "  leann build my-docs --docs ./documents                                  # Build index from directory\n",
            "  leann build my-code --docs ./src ./tests ./config                      # Build index from multiple directories\n",
            "  leann build my-files --docs ./file1.py ./file2.txt ./docs/             # Build index from files and directories\n",
            "  leann build my-mixed --docs ./readme.md ./src/ ./config.json           # Build index from mixed files/dirs\n",
            "  leann build my-ppts --docs ./ --file-types .pptx,.pdf                  # Index only PowerPoint and PDF files\n",
            "  leann search my-docs \"query\"                                           # Search in my-docs index\n",
            "  leann ask my-docs \"question\"                                           # Ask my-docs index\n",
            "  leann list                                                             # List all stored indexes\n",
            "  leann remove my-docs                                                   # Remove an index (local first, then global)\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTMkMtHUz6Dv",
        "outputId": "2e2aa9af-18ba-466e-c260-40eb6b6da7b0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 LEANN Indexes\n",
            "==================================================\n",
            "\n",
            "🏠 Current Project\n",
            "   /content/leann\n",
            "   ─────────────────────────────────────────────\n",
            "   1. 📁 my-docs ✅\n",
            "      📦 Size: 0.0 MB\n",
            "   2. 📁 .ipynb_checkpoints ❌\n",
            "   3. 📄 leann ✅\n",
            "      📦 Size: 0.1 MB\n",
            "\n",
            "==================================================\n",
            "📊 Total: 3 indexes across 1 projects\n",
            "\n",
            "💫 Quick start (current project):\n",
            "   leann search my-docs \"your query\"\n",
            "   leann ask my-docs --interactive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbs165Vm0dVZ",
        "outputId": "1198658c-af9a-4cea-b0ea-ddf25edb11fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                       ID              SIZE      MODIFIED          \n",
            "nomic-embed-text:latest    0a109f422b47    274 MB    43 minutes ago       \n",
            "llama3.2:1b                baf6a787fdff    1.3 GB    About an hour ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask my-docs \"What is the author's name as it appears in the context?\" --llm ollama --llm-model llama3.2:1b --embedding-model sentence-transformers/all-MiniLM-L6-v2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exLCxB-I0Ayp",
        "outputId": "32574106-5497-44f1-cfeb-86b3278824b2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: What is the author's name as it appears in the context? --llm-model llama3.2:1b --embedding-model sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python apps/document_rag.py --query \"What are the main techniques LEANN explores?\" \\\n",
        "  --index-dir hnswbuild --backend hnsw \\\n",
        "  --llm ollama --llm-model llama3.2:1b --thinking-budget high"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj04SU6p05Yo",
        "outputId": "d273833d-482d-49d4-eca4-9a72fe674773"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/leann/apps/document_rag.py\", line 12, in <module>\n",
            "    from base_rag_example import BaseRAGExample\n",
            "  File \"/content/leann/apps/base_rag_example.py\", line 14, in <module>\n",
            "    from leann.settings import resolve_ollama_host, resolve_openai_api_key, resolve_openai_base_url\n",
            "ModuleNotFoundError: No module named 'leann.settings'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/leann"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBtVHll42kSp",
        "outputId": "248c7568-bf7e-423a-b48f-62094bf17572"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/leann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask my-docs \\\n",
        "  --llm ollama \\\n",
        "  --llm-model llama3.2:1b \\\n",
        "  --query \"What is the author's name as it appears in the context??\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU8OfS6o1OOp",
        "outputId": "0cca6988-e6ff-4d9d-93c8-e06ca7a16c3e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --llm-model llama3.2:1b --query What is the author's name as it appears in the context??\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export LEANN_OLLAMA_HOST=\"http://localhost:11434\""
      ],
      "metadata": {
        "id": "P0zFRXDc2Lzo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toz9HmlE2Vzg",
        "outputId": "d2cb13c4-eea4-42f7-b436-24a373cdaa5b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                       ID              SIZE      MODIFIED          \n",
            "nomic-embed-text:latest    0a109f422b47    274 MB    About an hour ago    \n",
            "llama3.2:1b                baf6a787fdff    1.3 GB    About an hour ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/yichuan-w/LEANN/blob/main/docs/configuration-guide.md"
      ],
      "metadata": {
        "id": "_4rmsxco29Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask my-docs--query \"What techniques does LEANN use?\" --llm ollama --llm-model llama3.2:1b --embedding-mode ollama --embedding-model nomic-embed-text:latest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-knHJC_4kZL",
        "outputId": "0a1b8750-72d5-4093-940e-a3ab5ea8913e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: What techniques does LEANN use? --llm-model llama3.2:1b --embedding-mode ollama --embedding-model nomic-embed-text:latest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask -help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3IgGufZ40BA",
        "outputId": "a8e12d12-10d2-46f9-d590-1cc3b4506d57"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann ask [-h] [--llm {simulated,ollama,hf,openai}] [--model MODEL]\n",
            "                 [--host HOST] [--interactive] [--top-k TOP_K]\n",
            "                 [--complexity COMPLEXITY] [--beam-width BEAM_WIDTH]\n",
            "                 [--prune-ratio PRUNE_RATIO] [--recompute | --no-recompute]\n",
            "                 [--pruning-strategy {global,local,proportional}]\n",
            "                 [--thinking-budget {low,medium,high}]\n",
            "                 index_name\n",
            "\n",
            "positional arguments:\n",
            "  index_name            Index name\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --llm {simulated,ollama,hf,openai}\n",
            "                        LLM provider (default: ollama)\n",
            "  --model MODEL         Model name (default: qwen3:8b)\n",
            "  --host HOST\n",
            "  --interactive, -i     Interactive chat mode\n",
            "  --top-k TOP_K         Retrieval count (default: 20)\n",
            "  --complexity COMPLEXITY\n",
            "  --beam-width BEAM_WIDTH\n",
            "  --prune-ratio PRUNE_RATIO\n",
            "  --recompute, --no-recompute\n",
            "                        Enable/disable embedding recomputation during ask\n",
            "                        (default: enabled)\n",
            "  --pruning-strategy {global,local,proportional}\n",
            "  --thinking-budget {low,medium,high}\n",
            "                        Thinking budget for reasoning models\n",
            "                        (low/medium/high). Supported by GPT-Oss:20b and other\n",
            "                        reasoning models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs --llm ollama --model llama3.2:1b --query \"What is the author's name as it appears in the context?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq0KMfdo46R4",
        "outputId": "5238d146-8127-429b-aab0-70029f35623a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --query What is the author's name as it appears in the context?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs --model llama3.2:1b -- query \"What is the author's name as it appears in the context?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrfgt16h5j9R",
        "outputId": "9f3c0b86-c6cd-4824-df8b-ca496f54cb31"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: -- query What is the author's name as it appears in the context?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs --model llama3.2:1b --query \"What is the author’s name in the retrieved context?\" --top-k 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVEccEde51F6",
        "outputId": "9b8fbb52-8a0f-4334-b4e9-3382f2b16119"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --query What is the author’s name in the retrieved context?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs --model llama3.2:1b --query \"What is the author’s name as shown in the relevant passages?\" --top-k 3 --show-passages\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TssTCgzX6B5J",
        "outputId": "176e6790-6f0a-4442-83d0-3a9787284bf7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --query What is the author’s name as shown in the relevant passages? --show-passages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs \"What is the author’s name as it appears in the context?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWB9usT36LjR",
        "outputId": "bfb4470e-76fd-4485-f196-a08dd6ceeded"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: What is the author’s name as it appears in the context?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs --model llama3.2:1b \"What is the author’s name as it appears in the context?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoFu0Svg6Tdx",
        "outputId": "e4f3a3b6-86ea-42b2-b699-1e9b1443ae6a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: What is the author’s name as it appears in the context?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs \"What is the author’s name as it appears in the context?\" --model llama3.2:1b\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOkQrFTO6iBR",
        "outputId": "552c126f-972b-49e3-e37d-4b5721bff4a2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: What is the author’s name as it appears in the context?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# الصيغة تللك تعمل **جيدا**"
      ],
      "metadata": {
        "id": "hZrwgAH18pcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs --model llama3.2:1b --interactive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbrW3Pmj6ngJ",
        "outputId": "e11436f9-ba16-4ca5-df3b-aba41612c470"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chat with index '/content/leann/.leann/indexes/my-docs'...\n",
            "Using llama3.2:1b (ollama)\n",
            "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
            "[read_HNSW NL v4] Read levels vector, size: 1\n",
            "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
            "[read_HNSW NL v4] Read compact_level_ptr, size: 2\n",
            "[read_HNSW NL v4] Read compact_node_offsets, size: 2\n",
            "[read_HNSW NL v4] Read entry_point: 0, max_level: 0\n",
            "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
            "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 214\n",
            "[read_HNSW NL v4] Reading neighbors data into memory.\n",
            "[read_HNSW NL v4] Read neighbors data, size: 0\n",
            "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
            "INFO: Skipping external storage loading, since is_recompute is true.\n",
            "LEANN Assistant ready! Type 'quit' to exit\n",
            "========================================\n",
            "\n",
            "You: What is the author’s name\n",
            "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
            "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
            "ZmqDistanceComputer initialized: d=768, metric=0\n",
            "LEANN: Based on the provided context, I can confidently answer that the author's name is Jane Austen. The fact that it's an eBook with a release date (1998) and updates (2024), along with being part of a larger literary work (Pride and Prejudice), strongly suggests that it is indeed written by Jane Austen.\n",
            "\n",
            "You: Show me the existing text.\n",
            "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
            "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
            "ZmqDistanceComputer initialized: d=768, metric=0\n",
            "LEANN: Based on the provided context, I will attempt to display the existing text of \"Pride and Prejudice\" by Jane Austen:\n",
            "\n",
            "\"Not for the last time, Miss Bingley has been the object of my jealousy; but at present I am not so much affected by her beauty as by her good sense. Her father is a man of some property, but he is not in every respect advantageous to her family; though his abilities are better than his heart, and many people consider him great. His sister Jane is very pretty, and an excellent musician; she has a tolerable voice, which can scarce be expected to captivate the heart, but it may influence it. Her character is of a middle sort; not handsome, but not ugly; serious, reserved, and not very attractive; though her personal beauty, as I said before, is quite lost on me.\"\n",
            "\n",
            "Please note that this is an excerpt from the 1998 eBook edition of \"Pride and Prejudice\" with the most recent update being October 29, 2024.\n",
            "\n",
            "You: what is Release date?\n",
            "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
            "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
            "ZmqDistanceComputer initialized: d=768, metric=0\n",
            "LEANN: Based on the provided context, I'm going to take a close look at the information.\n",
            "\n",
            "The release date mentioned in the question is June 1, 1998. \n",
            "\n",
            "However, according to the text, the book was \"most recently updated\" on October 29, 2024. This implies that there have been changes or updates made to the text since the original publication of the book, specifically after January 1, 2025 (not 2024).\n",
            "\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/leann/data/PrideandPrejudice.txt\n",
        "/content/leann/.leann/indexes/my-docs"
      ],
      "metadata": {
        "id": "cHDOkyHM9Gtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs --model llama3.2:1b --interactive\n",
        "\n",
        "\n",
        "Starting chat with index '/content/leann/.leann/indexes/my-docs'...\n",
        "Using llama3.2:1b (ollama)\n",
        "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
        "[read_HNSW NL v4] Read levels vector, size: 1\n",
        "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
        "[read_HNSW NL v4] Read compact_level_ptr, size: 2\n",
        "[read_HNSW NL v4] Read compact_node_offsets, size: 2\n",
        "[read_HNSW NL v4] Read entry_point: 0, max_level: 0\n",
        "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
        "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 214\n",
        "[read_HNSW NL v4] Reading neighbors data into memory.\n",
        "[read_HNSW NL v4] Read neighbors data, size: 0\n",
        "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
        "INFO: Skipping external storage loading, since is_recompute is true.\n",
        "LEANN Assistant ready! Type 'quit' to exit\n",
        "========================================\n",
        "\n",
        "You: What is the author’s name\n",
        "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
        "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
        "ZmqDistanceComputer initialized: d=768, metric=0\n",
        "LEANN: Based on the provided context, I can confidently answer that the author's name is Jane Austen. The fact that it's an eBook with a release date (1998) and updates (2024), along with being part of a larger literary work (Pride and Prejudice), strongly suggests that it is indeed written by Jane Austen.\n",
        "\n",
        "You: Show me the existing text.\n",
        "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
        "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
        "ZmqDistanceComputer initialized: d=768, metric=0\n",
        "LEANN: Based on the provided context, I will attempt to display the existing text of \"Pride and Prejudice\" by Jane Austen:\n",
        "\n",
        "\"Not for the last time, Miss Bingley has been the object of my jealousy; but at present I am not so much affected by her beauty as by her good sense. Her father is a man of some property, but he is not in every respect advantageous to her family; though his abilities are better than his heart, and many people consider him great. His sister Jane is very pretty, and an excellent musician; she has a tolerable voice, which can scarce be expected to captivate the heart, but it may influence it. Her character is of a middle sort; not handsome, but not ugly; serious, reserved, and not very attractive; though her personal beauty, as I said before, is quite lost on me.\"\n",
        "\n",
        "Please note that this is an excerpt from the 1998 eBook edition of \"Pride and Prejudice\" with the most recent update being October 29, 2024.\n",
        "\n",
        "You: what is Release date?\n",
        "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
        "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
        "ZmqDistanceComputer initialized: d=768, metric=0\n",
        "LEANN: Based on the provided context, I'm going to take a close look at the information.\n",
        "\n",
        "The release date mentioned in the question is June 1, 1998.\n",
        "\n",
        "However, according to the text, the book was \"most recently updated\" on October 29, 2024. This implies that there have been changes or updates made to the text since the original publication of the book, specifically after January 1, 2025 (not 2024).\n",
        "\n",
        "You: exit\n",
        "Goodbye!\n",
        "\n",
        "النص\n",
        "Title: Pride and Prejudice\n",
        "\n",
        "Author: Jane Austen\n",
        "\n",
        "Release date: June 1, 1998 [eBook #1342]\n",
        "                Most recently updated: October 29, 2024\n",
        "\n",
        "Language: English\n",
        "\n",
        "\n",
        "الصيغة دى تعمل باقى الصيغ لاتعمل"
      ],
      "metadata": {
        "id": "Ga8WcCns9Crn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UqteJZYN9CpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9KOn4fhb9Cmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhgGLFS_9Cj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GjUZqWjM9CfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Release date"
      ],
      "metadata": {
        "id": "51ZI2OGT6tzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask /content/leann/.leann/indexes/my-docs --model llama3.2:1b --query\"What is the author’s name\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gXXJwiD8B-i",
        "outputId": "c59d567d-a20b-46b8-a739-8faa815d0cd4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --queryWhat is the author’s name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/leann/.leann/indexes/my-docs"
      ],
      "metadata": {
        "id": "KGwxpuLH_ZW1"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdI0RZqC--lL",
        "outputId": "b320ac32-392d-4976-92ea-fe230303f9dc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                       ID              SIZE      MODIFIED          \n",
            "nomic-embed-text:latest    0a109f422b47    274 MB    About an hour ago    \n",
            "llama3.2:1b                baf6a787fdff    1.3 GB    2 hours ago          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann build my-notes \\\n",
        "  --docs ./data \\\n",
        "  --embedding-mode ollama \\\n",
        "  --embedding-model nomic-embed-text:latest \\\n",
        "  --embedding-api-base http://127.0.0.1:11434/v1 \\\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59Q-S-w7-9EN",
        "outputId": "a4a6159f-a248-4fd6-ee7e-9084a5a72da0"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --embedding-api-base http://127.0.0.1:11434/v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann build my-notes --docs ./data --embedding-mode ollama --embedding-model nomic-embed-text:latest --embedding-api-base http://localhost:11434/v1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvdsXzxI-9Bb",
        "outputId": "f66596e7-7f33-45be-9958-5e6e514d6b10"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --embedding-api-base http://localhost:11434/v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "eYpV8BuTARyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!leann build my-notes --docs ./data --embedding-mode ollama --embedding-model nomic-embed-text:latest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJQoE0dK-8-C",
        "outputId": "ccd333a4-734b-4ec7-94fe-422fc39155d6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Indexing 1 path:\n",
            "  📁 Directories (1):\n",
            "    1. /content/leann/data\n",
            "Loading documents from 1 directory (1 total):\n",
            "  📁 Directories: data\n",
            "\n",
            "🔄 Processing 1 directory...\n",
            "Processing directory: data\n",
            "📋 No .gitignore found\n",
            "Loading files: 100% 1/1 [00:00<00:00,  1.30it/s]\n",
            "Loaded 1 documents from data\n",
            "start chunking documents\n",
            "Chunking documents: 100% 1/1 [00:00<00:00, 459.30doc/s]\n",
            "Loaded 1 documents, 1 chunks\n",
            "Building index 'my-notes' with hnsw backend...\n",
            "Writing passages: 100% 1/1 [00:00<00:00, 8738.13chunk/s]\n",
            "Computing Ollama embeddings: 100% 1/1 [00:00<00:00,  3.20it/s]\n",
            "M: 64 for level: 0\n",
            "Starting conversion: /content/leann/.leann/indexes/my-notes/documents.index -> /content/leann/.leann/indexes/my-notes/documents.csr.tmp\n",
            "[0.00s] Reading Index HNSW header...\n",
            "[0.00s]   Header read: d=768, ntotal=1\n",
            "[0.00s] Reading HNSW struct vectors...\n",
            "  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48\n",
            "[0.00s]   Read assign_probas (6)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28\n",
            "[0.23s]   Read cum_nneighbor_per_level (7)\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=1, Bytes=4\n",
            "[0.44s]   Read levels (1)\n",
            "[0.66s]   Probing for compact storage flag...\n",
            "[0.66s]   Found compact flag: False\n",
            "[0.66s]   Compact flag is False, reading original format...\n",
            "[0.66s]   Probing for potential extra byte before non-compact offsets...\n",
            "[0.66s]   Found and consumed an unexpected 0x00 byte.\n",
            "  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=2, Bytes=16\n",
            "[0.66s]   Read offsets (2)\n",
            "[0.87s]   Attempting to read neighbors vector...\n",
            "  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=64, Bytes=256\n",
            "[0.87s]   Read neighbors (64)\n",
            "[1.08s]   Read scalar params (ep=0, max_lvl=0)\n",
            "[1.08s] Checking for storage data...\n",
            "[1.08s]   Found storage fourcc: 49467849.\n",
            "[1.08s] Converting to CSR format...\n",
            "[1.08s]   Conversion loop finished.                        \n",
            "[1.08s] Running validation checks...\n",
            "    Checking total valid neighbor count...\n",
            "    OK: Total valid neighbors = 0\n",
            "    Checking final pointer indices...\n",
            "    OK: Final pointers match data size.\n",
            "[1.08s] Deleting original neighbors and offsets arrays...\n",
            "    CSR Stats: |data|=0, |level_ptr|=2\n",
            "[1.30s] Writing CSR HNSW graph data in FAISS-compatible order...\n",
            "   Pruning embeddings: Writing NULL storage marker.\n",
            "[1.53s] Conversion complete.\n",
            "Index built at /content/leann/.leann/indexes/my-notes/documents.leann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask my-notes \\\n",
        "  --llm ollama \\\n",
        "  --llm-model qwen3:14b \\\n",
        "  --host http://192.168.1.101:11434"
      ],
      "metadata": {
        "id": "5ZVN8Iw8APLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "what is Release date?--query\"What is the author’s name\""
      ],
      "metadata": {
        "id": "XwMQ2Qh2BDwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask my-notes --query \"What is the author’s name\"--llm ollama --llm-model llama3.2:1b --host http://localhost:11434"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcGk7kRMAco7",
        "outputId": "a3054f45-274c-4a74-a917-3ab08453bcfa"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --query What is the author’s name--llm ollama --llm-model llama3.2:1b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask my-notes --query \"What is the author’s name\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaoilmnQBBSy",
        "outputId": "5b5fc983-7f5e-4c3c-edf6-65c5b8f7abc0"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --query What is the author’s name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from leann import LeannSearcher\n",
        "\n",
        "searcher = LeannSearcher(\"/content/leann/.leann/indexes/my-notes/documents.leann.meta.json\")\n",
        "results = searcher.search(\"What is the author’s name\", top_k=10, recompute_embeddings=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "sZI79gGhBSBa",
        "outputId": "2f472268-05c2-41df-d65b-e7e673f5d0aa"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leann metadata file not found at /content/leann/.leann/indexes/my-notes/documents.leann.meta.json.meta.json, and you may need to rm -rf /content/leann/.leann/indexes/my-notes\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Leann metadata file not found at /content/leann/.leann/indexes/my-notes/documents.leann.meta.json.meta.json, \u001b[91m you may need to rm -rf /content/leann/.leann/indexes/my-notes\u001b[0m",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2770225242.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mleann\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLeannSearcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msearcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeannSearcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/leann/.leann/indexes/my-notes/documents.leann.meta.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is the author’s name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecompute_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/leann/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, index_path, enable_warmup, **backend_kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m             )\n\u001b[1;32m    621\u001b[0m             \u001b[0;31m# highlight in red the filenotfound error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m    623\u001b[0m                 \u001b[0;34mf\"Leann metadata file not found at {self.meta_path_str}, \\033[91m you may need to rm -rf {parent_dir}\\033[0m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Leann metadata file not found at /content/leann/.leann/indexes/my-notes/documents.leann.meta.json.meta.json, \u001b[91m you may need to rm -rf /content/leann/.leann/indexes/my-notes\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask my-notes --query \"What is the author’s name\" --llm ollama --llm-model llama3.2:1b --host \"http://localhost:11434\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrsv_oR3BnTC",
        "outputId": "d7263c11-8755-4d09-8d54-3f9912fe7576"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann [-h] {build,search,ask,list,remove} ...\n",
            "leann: error: unrecognized arguments: --query What is the author’s name --llm-model llama3.2:1b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask my-notes -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w438Pbs-B8fq",
        "outputId": "8c63eaa4-1b80-425b-976b-735ff63a27a4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: leann ask [-h] [--llm {simulated,ollama,hf,openai}] [--model MODEL]\n",
            "                 [--host HOST] [--interactive] [--top-k TOP_K]\n",
            "                 [--complexity COMPLEXITY] [--beam-width BEAM_WIDTH]\n",
            "                 [--prune-ratio PRUNE_RATIO] [--recompute | --no-recompute]\n",
            "                 [--pruning-strategy {global,local,proportional}]\n",
            "                 [--thinking-budget {low,medium,high}]\n",
            "                 index_name\n",
            "\n",
            "positional arguments:\n",
            "  index_name            Index name\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --llm {simulated,ollama,hf,openai}\n",
            "                        LLM provider (default: ollama)\n",
            "  --model MODEL         Model name (default: qwen3:8b)\n",
            "  --host HOST\n",
            "  --interactive, -i     Interactive chat mode\n",
            "  --top-k TOP_K         Retrieval count (default: 20)\n",
            "  --complexity COMPLEXITY\n",
            "  --beam-width BEAM_WIDTH\n",
            "  --prune-ratio PRUNE_RATIO\n",
            "  --recompute, --no-recompute\n",
            "                        Enable/disable embedding recomputation during ask\n",
            "                        (default: enabled)\n",
            "  --pruning-strategy {global,local,proportional}\n",
            "  --thinking-budget {low,medium,high}\n",
            "                        Thinking budget for reasoning models\n",
            "                        (low/medium/high). Supported by GPT-Oss:20b and other\n",
            "                        reasoning models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## شغالة"
      ],
      "metadata": {
        "id": "B6k0tYShDAo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!leann ask my-notes --llm ollama  --model llama3.2:1b -i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE32xMTOCDuz",
        "outputId": "d8a1bfd5-6ea5-4fb6-c133-9cc7b9be3601"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting chat with index 'my-notes'...\n",
            "Using llama3.2:1b (ollama)\n",
            "[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...\n",
            "[read_HNSW NL v4] Read levels vector, size: 1\n",
            "[read_HNSW NL v4] Reading Compact Storage format indices...\n",
            "[read_HNSW NL v4] Read compact_level_ptr, size: 2\n",
            "[read_HNSW NL v4] Read compact_node_offsets, size: 2\n",
            "[read_HNSW NL v4] Read entry_point: 0, max_level: 0\n",
            "[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e\n",
            "[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 214\n",
            "[read_HNSW NL v4] Reading neighbors data into memory.\n",
            "[read_HNSW NL v4] Read neighbors data, size: 0\n",
            "[read_HNSW NL v4] Finished reading metadata and CSR indices.\n",
            "INFO: Skipping external storage loading, since is_recompute is true.\n",
            "LEANN Assistant ready! Type 'quit' to exit\n",
            "========================================\n",
            "\n",
            "You: What is the author’s name\n",
            "WARNING:leann.api:  ⚠️  Requested top_k (20) exceeds total documents (1)\n",
            "WARNING:leann.api:  ✅ Auto-adjusted top_k to 1 to match available documents\n",
            "ZmqDistanceComputer initialized: d=768, metric=0\n",
            "LEANN: Based on the provided context, I can confidently state that the author's name is Jane Austen.\n",
            "\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "F6GKFmXUEWd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!leann build my-index --force --no-recompute --no-compact\n",
        "\n",
        "\n",
        "from leann import LeannSearcher\n",
        "\n",
        "searcher = LeannSearcher(\"/content/leann/.leann/indexes\")\n",
        "results = searcher.search(\"What is the author’s name\", top_k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmXCYXq7COsK",
        "outputId": "cf0bb5c7-59f3-4d64-a5a9-470aeb41da0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Indexing 1 path:\n",
            "  📁 Directories (1):\n",
            "    1. /content/leann\n",
            "Loading documents from 1 directory (1 total):\n",
            "  📁 Directories: .\n",
            "\n",
            "🔄 Processing 1 directory...\n",
            "Processing directory: .\n",
            "📋 Loaded .gitignore from . (includes all subdirectories)\n",
            "Loading files: 100% 116/116 [00:03<00:00, 30.27it/s]\n",
            "Loaded 109 documents from .\n",
            "start chunking documents\n",
            "Chunking documents: 100% 109/109 [00:02<00:00, 39.37doc/s]\n",
            "Loaded 109 documents, 1787 chunks\n",
            "Building index 'my-index' with hnsw backend...\n",
            "Warning: Skipping 4 empty/invalid text chunk(s). Processing 1783 valid chunks\n",
            "2025-09-25 04:08:01.889121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758773281.914566   49202 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758773281.922073   49202 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758773281.943792   49202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758773281.943839   49202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758773281.943845   49202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758773281.943850   49202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-25 04:08:01.950196: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Writing passages: 100% 1783/1783 [00:00<00:00, 35370.78chunk/s]\n",
            "Batches:   0% 0/56 [00:00<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!leann build my-index --force --no-recompute --no-compact\n",
        "\n",
        "\n",
        "from leann import LeannSearcher\n",
        "\n",
        "searcher = LeannSearcher(\"/content/leann/.leann/indexes/my-index\")\n",
        "results = searcher.search(\"What is the author’s name\", top_k=1)"
      ],
      "metadata": {
        "id": "kP-qYjb4DWd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python apps/document_rag.py --query \"What are the main techniques LEANN explores?\" \\\n",
        "  --index-dir /content/leann/.leann/indexes/my-notes \\\n",
        "  --llm ollama --llm-model gpt-oss:20b --thinking-budget high"
      ],
      "metadata": {
        "id": "QXeUqVwnEI9U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}